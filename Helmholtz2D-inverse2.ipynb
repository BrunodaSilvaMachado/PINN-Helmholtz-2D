{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-12T15:52:10.878644Z",
     "start_time": "2023-09-12T15:52:02.633270Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33142,
     "status": "ok",
     "timestamp": 1701869021507,
     "user": {
      "displayName": "Romulo Silva",
      "userId": "12525147468850690062"
     },
     "user_tz": 180
    },
    "id": "2f4eKpsFS_L8",
    "outputId": "0b316337-f7fa-4e21-cd14-b0e9f48024f0"
   },
   "outputs": [],
   "source": [
    "# Install the needed libraries\n",
    "%pip install --upgrade pip\n",
    "%pip install --upgrade \"jax[cuda]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html optax flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T16:51:52.699052Z",
     "start_time": "2023-09-13T16:51:51.754466Z"
    },
    "id": "JUE58xb0S_L-"
   },
   "outputs": [],
   "source": [
    "from typing import Callable, Tuple, Optional, Union, List, Any\n",
    "from dataclasses import dataclass, field\n",
    "import jax\n",
    "from jax import random, grad, vmap, jit, value_and_grad\n",
    "from functools import partial\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "from flax.training.train_state import TrainState\n",
    "import optax\n",
    "import time\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "j75qnHgTS_L-",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## The problem at hand\n",
    "\n",
    "We will now solve the 2D Helmholtz equation given by:\n",
    "\n",
    "$$\\Delta u + k^2 u = q(x, y), \\mbox{with } (x,y) \\in [-1,1]^2 $$\n",
    "\n",
    "with homogeneous Dirichlet boundary conditions.\n",
    "\n",
    "When $k$ is wave number, the forcing term is given by:\n",
    "\n",
    "$$ q(x,y) = 2 \\pi k \\cos{(\\pi k y)} \\sin{(\\pi k x)} + 2 \\pi k \\cos{(\\pi k x)} \\sin{(\\pi k y)} + k^2(x + y) \\sin{(\\pi k x)} \\sin{(\\pi k y)} - 2 \\pi^2 k^2 (x+y) \\sin{(\\pi k x)} \\sin{\\pi k y}$$\n",
    "\n",
    "while the exact solution is given by:\n",
    "\n",
    "$$ u(x, y) = (x+y) \\sin{(c_1 \\pi k x)} \\sin{(c_2 \\pi k y)}$$\n",
    "\n",
    "\n",
    "The example was taken from https://arxiv.org/abs/1906.01170"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "fCwYk5A4S_MA",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Approximating the system's behavior using a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função de Ativação personalizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_sigmoid_activation(x, alpha=0.1, beta=1.0):\n",
    "    \"\"\"\n",
    "    Função de ativação personalizada baseada em tanh e sigmoid.\n",
    "    \n",
    "    Parameters:\n",
    "        x (jnp.ndarray): Entrada da função.\n",
    "        alpha (float): Peso do componente sigmoid.\n",
    "        beta (float): Sensibilidade do componente sigmoid.\n",
    "    \n",
    "    Returns:\n",
    "        jnp.ndarray: Saída da função de ativação.\n",
    "    \"\"\"\n",
    "    tanh_part = jnp.tanh(x)\n",
    "    sigmoid_part = 1 / (1 + jnp.exp(-beta * x))\n",
    "    return tanh_part + alpha * sigmoid_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_silu_activation(x, alpha=0.1, beta=1.0):\n",
    "    tanh_part = jnp.tanh(x)\n",
    "    sigmoid_part = -beta*x / (1 + jnp.exp(-beta * x))\n",
    "    return tanh_part + alpha * sigmoid_part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codificador Posicional\n",
    "O codificador posicional transforma as entradas do modelo adicionando funções seno e cosseno de diferentes frequências. Para uma entrada \n",
    "$x$, o codificador gera:\n",
    "$$enc(x) = [x, sin(2^0 \\pi x), cos(2^0 \\pi x), sin(2^1 \\pi x), cos(2^1 \\pi x),...,sin(2^L \\pi x), cos(2^L \\pi x)]$$\n",
    "$L$ é o número de níveis harmônicos.\n",
    "\n",
    "Esse processo aumenta a capacidade do modelo de resolver padrões de alta frequência, como os encontrados em equações diferenciais como Helmholtz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(inputs, L=10):\n",
    "    \"\"\"\n",
    "    Aplica codificação posicional aos inputs.\n",
    "    \n",
    "    Parameters:\n",
    "        inputs: jnp.ndarray\n",
    "            Entrada original (e.g., x ou y).\n",
    "        L: int\n",
    "            Número de níveis de frequências harmônicas.\n",
    "\n",
    "    Returns:\n",
    "        jnp.ndarray:\n",
    "            Entrada codificada posicionalmente.\n",
    "    \"\"\"\n",
    "    encoded = [inputs]  # Incluímos o valor original\n",
    "    for i in range(L):\n",
    "        freq = 2 ** i * jnp.pi\n",
    "        encoded.append(jnp.sin(freq * inputs))\n",
    "        encoded.append(jnp.cos(freq * inputs))\n",
    "    return jnp.concatenate(encoded, axis=-1)  # Concatena tudo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "fH-ZE3DWS_MA"
   },
   "outputs": [],
   "source": [
    "rng_key_d = random.PRNGKey(4)\n",
    "def _get_activation(activation_name):\n",
    "    activations = {**jax.nn.__dict__, **jnp.__dict__}\n",
    "    activations[\"tanh_sigmoid\"] = tanh_sigmoid_activation\n",
    "    activations[\"tanh_silu\"] = tanh_silu_activation\n",
    "    if activation_name in activations:\n",
    "        return activations[activation_name]\n",
    "    else:\n",
    "        raise NotImplementedError(\"This activation function is not implemented yet!\")\n",
    "\n",
    "class DenseLayer(nn.Module):\n",
    "    features: int\n",
    "    kernel_init: Callable = nn.initializers.lecun_normal()\n",
    "    bias_init: Callable = nn.initializers.zeros\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        kernel = self.param(\n",
    "            \"kernel\",\n",
    "            self.kernel_init, \n",
    "            (x.shape[-1], self.features),\n",
    "        )\n",
    "\n",
    "        bias = self.param(\"bias\", self.bias_init, (self.features,))\n",
    "\n",
    "        return jnp.dot(x, kernel) + bias\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    arch_name: Optional[str] = \"Mlp\"\n",
    "    num_layers: int = 4\n",
    "    layer_size: int = 64\n",
    "    out_dim: int = 1\n",
    "    activation: str = \"tanh\"\n",
    "    lb: List = field(default_factory=List)\n",
    "    ub: List = field(default_factory=List)\n",
    "    harmonic_levels: int = 10 # Número de frequências no codificador posicional\n",
    "    extra_params: Any = None\n",
    "\n",
    "    def setup(self):\n",
    "        self.activation_fn = _get_activation(self.activation)\n",
    "        self.lb_array = jnp.asarray(self.lb)\n",
    "        self.ub_array = jnp.asarray(self.ub)\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, *inputs):\n",
    "        # Normalize the inputs\n",
    "        x = (\n",
    "            2.0 * (jnp.stack(inputs) - self.lb_array) / (self.ub_array - self.lb_array)\n",
    "           - 1.0\n",
    "        )\n",
    "        # Aplica codificação posicional\n",
    "        x = positional_encoding(x, L=self.harmonic_levels)\n",
    "        for _ in range(self.num_layers):\n",
    "            x = DenseLayer(features=self.layer_size)(x)\n",
    "            x = self.activation_fn(x)\n",
    "\n",
    "        # Output layer\n",
    "        x = DenseLayer(features=self.out_dim)(x)# inputs\n",
    "        return x.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "fWXp21VBS_MB",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## The batch sampler\n",
    "\n",
    "This time, let's implement a sampler so we can obtain the PDE residual evaluation points randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T16:52:01.819212Z",
     "start_time": "2023-09-13T16:52:01.815972Z"
    },
    "id": "_q8Adt7wS_MB"
   },
   "outputs": [],
   "source": [
    "class Helmholtz2DSampler(object):\n",
    "    def __init__(self, batch_size, dom_bds, rng_key):\n",
    "        self.batch_size = batch_size\n",
    "        self.key = rng_key\n",
    "        self.dom_bds = dom_bds\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        self.key, subkey = random.split(self.key)\n",
    "        batch = self.data_generation(subkey)\n",
    "        return batch\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def data_generation(self, key):\n",
    "        key1, key2 = random.split(key)\n",
    "        # Points for evaluating the PDE residual\n",
    "        x = random.uniform(\n",
    "            key1,\n",
    "            shape=(self.batch_size,),\n",
    "            minval=self.dom_bds[\"xmin\"],\n",
    "            maxval=self.dom_bds[\"xmax\"],\n",
    "        )\n",
    "        y = random.uniform(\n",
    "            key2,\n",
    "            shape=(self.batch_size,),\n",
    "            minval=self.dom_bds[\"ymin\"],\n",
    "            maxval=self.dom_bds[\"ymax\"],\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"x\": x,\n",
    "            \"y\": y,\n",
    "            \"xmin\": self.dom_bds[\"xmin\"],\n",
    "            \"xmax\": self.dom_bds[\"xmax\"],\n",
    "            \"ymin\": self.dom_bds[\"ymin\"],\n",
    "            \"ymax\": self.dom_bds[\"ymax\"],\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agendador de taxa de aprendizado\n",
    "\n",
    "Agendador de taxa de aprendizado (learning rate schedule) com aquecimento (warmup) e decaimento exponencial (exponential decay):\n",
    "\n",
    "**Implementação do Scheduler**\n",
    "\n",
    "Use a função optax.join_schedules para combinar um período de warmup com o decaimento exponencial.\n",
    "\n",
    "**Explicação**\n",
    "\n",
    "* __Warmup__ (`linear_schedule`): Aumenta linearmente a taxa de aprendizado de `init_value` para `peak_value` nos primeiros `warmup_steps`.\n",
    "\n",
    "* __Decaimento Exponencial__ (`exponential_decay`): Após o período de warmup, a taxa de aprendizado decai exponencialmente com a fórmula:\n",
    "\n",
    "    $learning\\ rate = peak\\ value \\cdot (decay\\ rate)^{step/decay\\ steps}$\n",
    " \n",
    "* __Combinação__ (`join_schedules`): Une os dois agendadores e aplica o warmup antes do decaimento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_warmup_exponential_decay_schedule(\n",
    "    init_value=1e-6,\n",
    "    peak_value=1e-3,\n",
    "    warmup_steps=1000,\n",
    "    decay_rate=0.98,\n",
    "    decay_steps=5000,\n",
    "):\n",
    "    \"\"\"\n",
    "    Cria um agendador de taxa de aprendizado com warmup seguido por decaimento exponencial.\n",
    "    \"\"\"\n",
    "    # Agendador para a fase de warmup\n",
    "    warmup_schedule = optax.linear_schedule(\n",
    "        init_value=init_value,\n",
    "        end_value=peak_value,\n",
    "        transition_steps=warmup_steps,\n",
    "    )\n",
    "\n",
    "    # Agendador para o decaimento exponencial\n",
    "    decay_schedule = optax.exponential_decay(\n",
    "        init_value=peak_value,\n",
    "        transition_steps=decay_steps,\n",
    "        decay_rate=decay_rate,\n",
    "    )\n",
    "\n",
    "    # Combinar warmup e decaimento exponencial\n",
    "    schedule = optax.join_schedules(\n",
    "        schedules=[warmup_schedule, decay_schedule],\n",
    "        boundaries=[warmup_steps],\n",
    "    )\n",
    "\n",
    "    return schedule\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T16:52:20.277631Z",
     "start_time": "2023-09-13T16:52:20.276323Z"
    },
    "id": "Bvidi6hLS_MC"
   },
   "outputs": [],
   "source": [
    "# Define the train step function\n",
    "@jit\n",
    "def train_step(state, batch, k, u_max):\n",
    "\n",
    "    def pde_residual_fn(params, x, y):\n",
    "\n",
    "        u = state.apply_fn(params, x, y)\n",
    "        u_xx = grad(grad(state.apply_fn, 1), 1)(params, x, y)\n",
    "        u_yy = grad(grad(state.apply_fn, 2), 2)(params, x, y)\n",
    "\n",
    "        #smoothness_penalty = 0.01*(jnp.mean(jnp.square(u_xx)) + jnp.mean(jnp.square(u_yy)))\n",
    "\n",
    "        ksq = params[\"params\"][\"ksq\"]\n",
    "        k_aprox = jnp.sqrt(ksq)\n",
    "        q = (\n",
    "            2.0 * jnp.pi * k_aprox * jnp.cos(jnp.pi * k_aprox * y) * jnp.sin(jnp.pi * k_aprox * x)\n",
    "            + 2.0 * jnp.pi * k_aprox * jnp.cos(jnp.pi * k_aprox * x) * jnp.sin(jnp.pi * k_aprox * y)\n",
    "            + k_aprox**2*(x + y) * jnp.sin(jnp.pi * k_aprox * x) * jnp.sin(jnp.pi * k_aprox * y)\n",
    "            - 2.0 * jnp.pi**2 * k_aprox**2 * (x + y) * jnp.sin(jnp.pi * k_aprox * x) * jnp.sin(jnp.pi * k_aprox * y)\n",
    "        )/u_max\n",
    "        \n",
    "        res = (u_xx + u_yy) + ksq * u - q #+ smoothness_penalty\n",
    "        return res, u\n",
    "\n",
    "    # Define the loss function\n",
    "    def loss_fn(params, batch):\n",
    "\n",
    "        res_pred, u_pred = vmap(pde_residual_fn, (None, 0, 0))(\n",
    "            params, batch[\"x\"], batch[\"y\"]\n",
    "        )\n",
    "        pde_loss = jnp.square(res_pred).mean()\n",
    "\n",
    "        u_true = vmap(\n",
    "            lambda x, y: (x + y) * jnp.sin(jnp.pi * k *x) * jnp.sin(jnp.pi * k * y), (0, 0)\n",
    "        )(batch[\"x\"], batch[\"y\"])\n",
    "        u_true_norm = u_true/jnp.max(u_true)\n",
    "\n",
    "        u_west = vmap(state.apply_fn, (None, None, 0))(\n",
    "            params, batch[\"xmin\"], batch[\"y\"]\n",
    "        )\n",
    "\n",
    "        u_east = vmap(state.apply_fn, (None, None, 0))(\n",
    "            params, batch[\"xmax\"], batch[\"y\"]\n",
    "        )\n",
    "\n",
    "        u_north = vmap(state.apply_fn, (None, 0, None))(\n",
    "            params, batch[\"x\"], batch[\"ymax\"]\n",
    "        )\n",
    "\n",
    "        u_south = vmap(state.apply_fn, (None, 0, None))(\n",
    "            params, batch[\"x\"], batch[\"ymin\"]\n",
    "        )\n",
    "\n",
    "        data_loss = (\n",
    "            jnp.square(u_pred - u_true_norm).mean()\n",
    "            + jnp.square(u_west).mean()\n",
    "            + jnp.square(u_east).mean()\n",
    "            + jnp.square(u_north).mean()\n",
    "            + jnp.square(u_south).mean()\n",
    "        )\n",
    "\n",
    "        k_loss = jnp.abs(params[\"params\"][\"ksq\"] - k**2)\n",
    "\n",
    "        total_loss = 20.0 * data_loss + pde_loss + 10.0*k_loss\n",
    "\n",
    "        return total_loss, {\n",
    "            \"total_loss\": total_loss,\n",
    "            \"data_loss\": data_loss,\n",
    "            \"pde_loss\": pde_loss,\n",
    "            \"k_loss\": k_loss,\n",
    "        }\n",
    "\n",
    "    # Compute the loss and its grads w.r.t. the model parameters\n",
    "    (_, loss_components), grads = value_and_grad(loss_fn, has_aux=True)(\n",
    "        state.params, batch\n",
    "    )\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return loss_components, state\n",
    "\n",
    "\n",
    "@jit\n",
    "def eval_step(state, batch, x, y):\n",
    "    def l2_error(params, batch):\n",
    "        u_pred = vmap(vmap(state.apply_fn, (None, None, 0)), (None, 0, None))(\n",
    "            params, x, y\n",
    "        )\n",
    "        return jnp.linalg.norm(x=(u_pred.flatten() - batch[\"u\"].flatten()), ord=2) / jnp.linalg.norm(\n",
    "            x=batch[\"u\"].flatten(), ord=2\n",
    "        )\n",
    "\n",
    "    return l2_error(state.params, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helmholtz_2d(k = 1, n = 101, xb = [-1, 1], yb=[-1, 1]):\n",
    "    u_true_fn = lambda x, y: (x + y) * jnp.sin(jnp.pi * k * x) * jnp.sin(jnp.pi * k * y)\n",
    "\n",
    "    du_dx_0_fn = lambda y: jnp.pi * k * y * jnp.sin(jnp.pi * k * y)\n",
    "    du_dy_0_fn = lambda x: jnp.pi * k * x * jnp.sin(jnp.pi * k * x)\n",
    "\n",
    "    q_fn = (\n",
    "        lambda x, y: 2.0 * jnp.pi *k * jnp.cos(jnp.pi * k * y) * jnp.sin(jnp.pi * k * x)\n",
    "        + 2.0 * jnp.pi * k * jnp.cos(jnp.pi * k * x) * jnp.sin(jnp.pi * k * y)\n",
    "        + k**2 * (x + y) * jnp.sin(jnp.pi * k * x) * jnp.sin(jnp.pi * k * y)\n",
    "        - 2.0 * jnp.pi**2 * k**2 * (x + y) * jnp.sin(jnp.pi * k * x) * jnp.sin(jnp.pi * k * y)\n",
    "    )\n",
    "\n",
    "    x = jnp.linspace(xb[0], xb[1], n)\n",
    "    y = jnp.linspace(yb[0], yb[1], n)\n",
    "\n",
    "    u_true = vmap(vmap(u_true_fn, (None, 0)), (0, None))(x, y)\n",
    "    q = vmap(vmap(q_fn, (None, 0)), (0, None))(x, y)\n",
    "    u_max = jnp.max(u_true)\n",
    "    q_max = jnp.max(q)\n",
    "    return {\n",
    "        \"x\": x,\n",
    "        \"y\": y,\n",
    "        \"u_true\": u_true,\n",
    "        \"u_true_norm\": u_true / u_max,\n",
    "        \"u_max\": u_max,\n",
    "        \"forcing_term\": q,\n",
    "        \"q_max\": q_max\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T16:53:09.669319Z",
     "start_time": "2023-09-13T16:53:06.548292Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 613
    },
    "executionInfo": {
     "elapsed": 2547,
     "status": "ok",
     "timestamp": 1701871140047,
     "user": {
      "displayName": "Romulo Silva",
      "userId": "12525147468850690062"
     },
     "user_tz": 180
    },
    "id": "uvnZoivZS_MC",
    "outputId": "98396f93-e62e-4952-d416-2d24fdc73f1f"
   },
   "outputs": [],
   "source": [
    "def helmholtz_graf(x, y, u_true, u_pred, k, k_pred, ep, timelapse):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(10, 10 / 1.618))\n",
    "    \n",
    "    im = axs[0].contourf(x, y, u_true, levels=128, cmap=\"jet\")\n",
    "    fig.colorbar(im, ax=axs[0], location='bottom', ticks=np.linspace(u_true.min(), u_true.max(), 5))\n",
    "    axs[0].set_title(r\"$u_{true}(x,y)$\")\n",
    "    \n",
    "    im = axs[1].contourf(x, y, u_pred, levels=128, cmap=\"jet\")\n",
    "    fig.colorbar(im, ax=axs[1], location='bottom', ticks=np.linspace(u_pred.min(), u_pred.max(), 5))\n",
    "    axs[1].set_title(r\"$u_{pred}(x,y)$\")\n",
    "    \n",
    "    error = jnp.abs(u_pred-u_true)\n",
    "    im = axs[2].contourf(x, y, error, levels=128, cmap=\"jet\")\n",
    "    fig.colorbar(im, ax=axs[2], location='bottom', ticks=np.linspace(error.min(), error.max(), 5))\n",
    "    axs[2].set_title(r\"$|err(x,y)|$\")\n",
    "    \n",
    "    for ax in axs:\n",
    "        ax.set_aspect(\"equal\")\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    fig.suptitle(\"Prediction AFTER training the model\")\n",
    "    plt.savefig(f'results/inv_helmholtz_k{k}EPL{ep}.jax.png')\n",
    "\n",
    "    a = sum(error)/len(error)\n",
    "    b = sum(a)/len(a)\n",
    "\n",
    "    formatted_time = str(datetime.timedelta(seconds=int(timelapse)))\n",
    "    \n",
    "    print(f\"time:{formatted_time} error: {b}\")\n",
    "    with open(f\"results/inv_helmholtz_k{k}EPL{ep}.log\",\"w\") as file:\n",
    "        file.write(f\"time:{formatted_time} error: {b} k: {k_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T16:53:15.562585Z",
     "start_time": "2023-09-13T16:53:15.168868Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "executionInfo": {
     "elapsed": 1852,
     "status": "ok",
     "timestamp": 1701871189943,
     "user": {
      "displayName": "Romulo Silva",
      "userId": "12525147468850690062"
     },
     "user_tz": 180
    },
    "id": "hzUQVKt1S_MC",
    "outputId": "999d7377-b390-4b99-cb07-0ba85f30c610"
   },
   "outputs": [],
   "source": [
    "def helmholtz_error_graf(total_loss_log, data_loss_log, pde_loss_log, error_log, k_log, k, ep):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 10 / 1.618))\n",
    "    epoch_log = jnp.linspace(0, ep, len(total_loss_log))\n",
    "    axs[0].plot(epoch_log, total_loss_log, label=\"Total Loss\")\n",
    "    axs[0].plot(epoch_log, data_loss_log, label=\"Data Loss\")\n",
    "    axs[0].plot(epoch_log, pde_loss_log, label=\"PDE Loss\")\n",
    "    axs[0].plot(epoch_log, error_log, label=\"Error\")\n",
    "    axs[1].plot(epoch_log, k_log, label=\"k appox\")\n",
    "    \n",
    "    axs[0].set_yscale(\"log\")\n",
    "    axs[0].legend()\n",
    "    axs[1].legend()\n",
    "    plt.savefig(f'results/inv_helmholtz_error_k{k}EPL{ep}.jax.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def helmholtz_init_graf(x,y,u_true,q):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 10 / 1.618), sharex=True, sharey=True)\n",
    "    \n",
    "    axs[0].contourf(x, y, u_true, levels=128, cmap=\"jet\")\n",
    "    axs[0].set_title(r\"$u_{true}(x,y)$\")\n",
    "    axs[1].contourf(x, y, q, levels=128, cmap=\"jet\")\n",
    "    axs[1].set_title(r\"$q(x,y)$\")\n",
    "    \n",
    "    for ax in axs:\n",
    "        ax.set_aspect(\"equal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T16:52:57.670116Z",
     "start_time": "2023-09-13T16:52:24.099735Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40641,
     "status": "ok",
     "timestamp": 1701871131330,
     "user": {
      "displayName": "Romulo Silva",
      "userId": "12525147468850690062"
     },
     "user_tz": 180
    },
    "id": "zK3LxBvZS_MC",
    "outputId": "9099ec9a-0c91-489b-f3af-350689ff483f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def multigrid_training(state, rng_key, k, resolutions, n_ep = 5000, xdom=[-1,1], ydom=[-1,1]):\n",
    "    total_loss_log = []\n",
    "    data_loss_log = []\n",
    "    pde_loss_log = []\n",
    "    epoch_log = []\n",
    "    error_log = []\n",
    "    k_log = []\n",
    "    \n",
    "    #####################################################################\n",
    "    batch_acc = len(resolutions) - 1 if len(resolutions) > 1 else 1\n",
    "    rng_key, subkey = random.split(rng_key)\n",
    "    start = time.time()\n",
    "    # Loop over resolutions\n",
    "    for resolution in resolutions:\n",
    "        print(f\"Training at resolution: {resolution}x{resolution}\")\n",
    "\n",
    "        # Generate data for this resolution\n",
    "        hmt = helmholtz_2d(k, resolution)\n",
    "        x, y, u_true_n, u_max = hmt[\"x\"], hmt[\"y\"], hmt[\"u_true_norm\"], hmt[\"u_max\"]\n",
    "\n",
    "        sampler = Helmholtz2DSampler(\n",
    "            batch_size=int(256 + 2*resolution/(batch_acc)) ,\n",
    "            dom_bds={\"xmin\": xdom[0], \"xmax\": xdom[1], \"ymin\": ydom[0], \"ymax\": ydom[1]},\n",
    "            rng_key=subkey,\n",
    "        )\n",
    "        batch_iterator = iter(sampler)\n",
    "\n",
    "        eval_batch = {\"x\": x, \"y\": y, \"u\": u_true_n}\n",
    "        for epoch in range(n_ep):\n",
    "            train_batch = next(batch_iterator)\n",
    "            loss, state = train_step(state, train_batch, k, u_max)\n",
    "            if epoch % 200 == 0:\n",
    "                l2_error = eval_step(state, eval_batch, x, y)\n",
    "                ksq = jnp.sqrt(state.params[\"params\"][\"ksq\"])\n",
    "                epoch_log.append(epoch)\n",
    "                total_loss_log.append(loss[\"total_loss\"])\n",
    "                data_loss_log.append(loss[\"data_loss\"])\n",
    "                pde_loss_log.append(loss[\"pde_loss\"])\n",
    "                error_log.append(l2_error)\n",
    "                k_log.append(ksq)\n",
    "                # l2_error = 0\n",
    "                print(\n",
    "                    f\"Epoch: {epoch} -- Total Loss: {loss['total_loss']}  -- Data Loss: {loss['data_loss']} -- PDE Loss: {loss['pde_loss']}  -- Error: {l2_error} -- k: {ksq}\"\n",
    "                )\n",
    "    \n",
    "    hmt[\"timelapse\"] = time.time() - start\n",
    "    hmt[\"u_pred\"] = vmap(vmap(state.apply_fn, (None, None, 0)), (None, 0, None))(\n",
    "        state.params, x, y\n",
    "    )\n",
    "    hmt[\"k_pred\"] = ksq\n",
    "    ret = {\n",
    "        \"total_loss_log\":total_loss_log,\n",
    "        \"data_loss_log\":data_loss_log,\n",
    "        \"pde_loss_log\":pde_loss_log,\n",
    "        \"epoch_log\":epoch_log,\n",
    "        \"error_log\":error_log,\n",
    "        \"k_log\":k_log \n",
    "    }\n",
    "    return hmt, ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the random key\n",
    "rng_key = random.PRNGKey(42)\n",
    "\n",
    "class Helmholtz2DInv(Mlp):\n",
    "    def setup(self):\n",
    "        super().setup()\n",
    "        self.ksq = self.param(\"ksq\", jax.nn.initializers.uniform(scale=12.0) , ())\n",
    "\n",
    "# Define domain\n",
    "xmin, xmax = -1, 1\n",
    "ymin, ymax = -1, 1\n",
    "\n",
    "# Define the Neural Network\n",
    "model = Helmholtz2DInv(num_layers=5, layer_size=256, out_dim=1, activation = \"tanh\",\n",
    "            lb=[xmin, ymin], ub=[xmax, ymax], harmonic_levels=5)\n",
    "\n",
    "# Initialize the model parameters\n",
    "rng_key, init_key = random.split(rng_key, 2)\n",
    "dummy_input = jnp.asarray(0.0)\n",
    "params = model.init(init_key, dummy_input, dummy_input)\n",
    "\n",
    "## Define the optimizer using Optax\n",
    "# The learning rate scheduler\n",
    "lr = create_warmup_exponential_decay_schedule(init_value=1e-6, peak_value=1e-3, \n",
    "                                              warmup_steps=500,decay_steps=2000, decay_rate=0.98)\n",
    "tx = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),\n",
    "    optax.adam(learning_rate=lr)\n",
    ")\n",
    "\n",
    "# Create the training state\n",
    "state = TrainState.create(\n",
    "    apply_fn=lambda params_, x_, y_: model.apply(\n",
    "        params_, x_, y_)[0],\n",
    "    params=params,\n",
    "    tx=tx,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training at resolution: 100x100\n",
      "Epoch: 0 -- Total Loss: 253820.4375  -- Data Loss: 1.8122373819351196 -- PDE Loss: 253774.28125  -- Error: 2.1455180644989014 -- k: 4.899999618530273\n",
      "Epoch: 200 -- Total Loss: 15111.7783203125  -- Data Loss: 0.10454420745372772 -- PDE Loss: 15099.5732421875  -- Error: 0.9824780225753784 -- k: 4.897787570953369\n",
      "Epoch: 400 -- Total Loss: 2852.4423828125  -- Data Loss: 0.36923184990882874 -- PDE Loss: 2833.946533203125  -- Error: 0.5754085183143616 -- k: 4.887540340423584\n",
      "Epoch: 600 -- Total Loss: 1353.5364990234375  -- Data Loss: 0.38158267736434937 -- PDE Loss: 1333.6580810546875  -- Error: 0.621303379535675 -- k: 4.8759613037109375\n",
      "Epoch: 800 -- Total Loss: 791.5511474609375  -- Data Loss: 0.39656874537467957 -- PDE Loss: 770.7386474609375  -- Error: 0.5428304672241211 -- k: 4.869405269622803\n",
      "Epoch: 1000 -- Total Loss: 497.128662109375  -- Data Loss: 0.43558061122894287 -- PDE Loss: 475.159423828125  -- Error: 0.5777698159217834 -- k: 4.865593433380127\n",
      "Epoch: 1200 -- Total Loss: 428.82232666015625  -- Data Loss: 0.415459007024765 -- PDE Loss: 406.8330383300781  -- Error: 0.5637261867523193 -- k: 4.861283302307129\n",
      "Epoch: 1400 -- Total Loss: 455.4267578125  -- Data Loss: 0.37403789162635803 -- PDE Loss: 434.03875732421875  -- Error: 0.5691045522689819 -- k: 4.858914852142334\n",
      "Epoch: 1600 -- Total Loss: 402.0929260253906  -- Data Loss: 0.4171752631664276 -- PDE Loss: 379.5167541503906  -- Error: 0.5768720507621765 -- k: 4.855539798736572\n",
      "Epoch: 1800 -- Total Loss: 496.5084228515625  -- Data Loss: 0.34411269426345825 -- PDE Loss: 475.2779235839844  -- Error: 0.6041318774223328 -- k: 4.854365825653076\n",
      "Epoch: 2000 -- Total Loss: 176.18283081054688  -- Data Loss: 0.36858877539634705 -- PDE Loss: 154.28526306152344  -- Error: 0.5583450198173523 -- k: 4.852545261383057\n",
      "Epoch: 2200 -- Total Loss: 195.8131866455078  -- Data Loss: 0.32753026485443115 -- PDE Loss: 174.53382873535156  -- Error: 0.5461151003837585 -- k: 4.850481033325195\n",
      "Epoch: 2400 -- Total Loss: 225.7818145751953  -- Data Loss: 0.38676780462265015 -- PDE Loss: 203.197998046875  -- Error: 0.5744033455848694 -- k: 4.849226951599121\n",
      "Epoch: 2600 -- Total Loss: 236.382568359375  -- Data Loss: 0.3561907708644867 -- PDE Loss: 214.36679077148438  -- Error: 0.5470984578132629 -- k: 4.848776340484619\n",
      "Epoch: 2800 -- Total Loss: 228.62965393066406  -- Data Loss: 0.3610777258872986 -- PDE Loss: 206.49720764160156  -- Error: 0.6136538982391357 -- k: 4.84860372543335\n",
      "Epoch: 3000 -- Total Loss: 147.91781616210938  -- Data Loss: 0.3423832952976227 -- PDE Loss: 126.13058471679688  -- Error: 0.5520353317260742 -- k: 4.848339080810547\n",
      "Epoch: 3200 -- Total Loss: 186.708251953125  -- Data Loss: 0.3020336627960205 -- PDE Loss: 165.65049743652344  -- Error: 0.5482739806175232 -- k: 4.84753942489624\n",
      "Epoch: 3400 -- Total Loss: 141.37478637695312  -- Data Loss: 0.32622113823890686 -- PDE Loss: 119.8456039428711  -- Error: 0.5663189888000488 -- k: 4.8476152420043945\n",
      "Epoch: 3600 -- Total Loss: 171.25048828125  -- Data Loss: 0.3139849901199341 -- PDE Loss: 149.9579315185547  -- Error: 0.5765553116798401 -- k: 4.8475661277771\n",
      "Epoch: 3800 -- Total Loss: 122.5752182006836  -- Data Loss: 0.2667415738105774 -- PDE Loss: 102.22840881347656  -- Error: 0.5350115895271301 -- k: 4.847560405731201\n",
      "Epoch: 4000 -- Total Loss: 114.36671447753906  -- Data Loss: 0.3261702060699463 -- PDE Loss: 92.8165283203125  -- Error: 0.6042436361312866 -- k: 4.8474225997924805\n",
      "Epoch: 4200 -- Total Loss: 128.31488037109375  -- Data Loss: 0.27446457743644714 -- PDE Loss: 107.80699920654297  -- Error: 0.5175275802612305 -- k: 4.8474931716918945\n",
      "Epoch: 4400 -- Total Loss: 153.21011352539062  -- Data Loss: 0.25035393238067627 -- PDE Loss: 133.21556091308594  -- Error: 0.5120149254798889 -- k: 4.847805976867676\n",
      "Epoch: 4600 -- Total Loss: 149.81124877929688  -- Data Loss: 0.25940805673599243 -- PDE Loss: 129.6988983154297  -- Error: 0.5310885906219482 -- k: 4.84846305847168\n",
      "Epoch: 4800 -- Total Loss: 124.53609466552734  -- Data Loss: 0.26991498470306396 -- PDE Loss: 104.22118377685547  -- Error: 0.5166236758232117 -- k: 4.848516464233398\n",
      "Training at resolution: 200x200\n",
      "Epoch: 0 -- Total Loss: 122.46915435791016  -- Data Loss: 0.252699077129364 -- PDE Loss: 102.50897979736328  -- Error: 0.5109430551528931 -- k: 4.848639488220215\n",
      "Epoch: 200 -- Total Loss: 77.52218627929688  -- Data Loss: 0.21994154155254364 -- PDE Loss: 58.33911895751953  -- Error: 0.4911292493343353 -- k: 4.849935054779053\n",
      "Epoch: 400 -- Total Loss: 98.23692321777344  -- Data Loss: 0.20807860791683197 -- PDE Loss: 79.41565704345703  -- Error: 0.4817911982536316 -- k: 4.851169109344482\n",
      "Epoch: 600 -- Total Loss: 104.02285766601562  -- Data Loss: 0.22151479125022888 -- PDE Loss: 85.02410125732422  -- Error: 0.47889411449432373 -- k: 4.852132797241211\n",
      "Epoch: 800 -- Total Loss: 124.59046936035156  -- Data Loss: 0.25696641206741333 -- PDE Loss: 105.00016784667969  -- Error: 0.5280209183692932 -- k: 4.853342533111572\n",
      "Epoch: 1000 -- Total Loss: 105.5280990600586  -- Data Loss: 0.22914543747901917 -- PDE Loss: 86.5714111328125  -- Error: 0.5626372694969177 -- k: 4.8541460037231445\n",
      "Epoch: 1200 -- Total Loss: 101.60267639160156  -- Data Loss: 0.1920982301235199 -- PDE Loss: 83.49015808105469  -- Error: 0.5201433300971985 -- k: 4.855182647705078\n",
      "Epoch: 1400 -- Total Loss: 127.5539779663086  -- Data Loss: 0.2526436448097229 -- PDE Loss: 108.34758758544922  -- Error: 0.5467889904975891 -- k: 4.8564043045043945\n",
      "Epoch: 1600 -- Total Loss: 82.76953125  -- Data Loss: 0.1939995139837265 -- PDE Loss: 64.8338851928711  -- Error: 0.4763420522212982 -- k: 4.857424736022949\n",
      "Epoch: 1800 -- Total Loss: 142.38864135742188  -- Data Loss: 0.1958448886871338 -- PDE Loss: 124.558837890625  -- Error: 0.5457700490951538 -- k: 4.858847618103027\n",
      "Epoch: 2000 -- Total Loss: 83.302978515625  -- Data Loss: 0.20750033855438232 -- PDE Loss: 65.35343170166016  -- Error: 0.5077276825904846 -- k: 4.8600616455078125\n",
      "Epoch: 2200 -- Total Loss: 105.99549865722656  -- Data Loss: 0.16695474088191986 -- PDE Loss: 88.98603820800781  -- Error: 0.44152599573135376 -- k: 4.861374855041504\n",
      "Epoch: 2400 -- Total Loss: 64.6627197265625  -- Data Loss: 0.16540159285068512 -- PDE Loss: 47.842891693115234  -- Error: 0.4425547420978546 -- k: 4.862988471984863\n",
      "Epoch: 2600 -- Total Loss: 104.04387664794922  -- Data Loss: 0.17369933426380157 -- PDE Loss: 87.22493743896484  -- Error: 0.4584561288356781 -- k: 4.86472749710083\n",
      "Epoch: 2800 -- Total Loss: 60.7498779296875  -- Data Loss: 0.1748127043247223 -- PDE Loss: 44.071231842041016  -- Error: 0.4417346715927124 -- k: 4.866421222686768\n",
      "Epoch: 3000 -- Total Loss: 105.41706848144531  -- Data Loss: 0.16792084276676178 -- PDE Loss: 89.0324935913086  -- Error: 0.4269801080226898 -- k: 4.8679938316345215\n",
      "Epoch: 3200 -- Total Loss: 111.22926330566406  -- Data Loss: 0.16847972571849823 -- PDE Loss: 95.02189636230469  -- Error: 0.4588645398616791 -- k: 4.869949817657471\n",
      "Epoch: 3400 -- Total Loss: 93.4069595336914  -- Data Loss: 0.15341703593730927 -- PDE Loss: 77.64082336425781  -- Error: 0.4287654459476471 -- k: 4.871365070343018\n",
      "Epoch: 3600 -- Total Loss: 75.838623046875  -- Data Loss: 0.16269010305404663 -- PDE Loss: 60.0481071472168  -- Error: 0.4173550307750702 -- k: 4.873033046722412\n",
      "Epoch: 3800 -- Total Loss: 43.67476272583008  -- Data Loss: 0.12452860176563263 -- PDE Loss: 28.7933292388916  -- Error: 0.3851751983165741 -- k: 4.874533176422119\n",
      "Epoch: 4000 -- Total Loss: 82.31686401367188  -- Data Loss: 0.13493561744689941 -- PDE Loss: 67.42044067382812  -- Error: 0.424725204706192 -- k: 4.876479625701904\n",
      "Epoch: 4200 -- Total Loss: 86.39985656738281  -- Data Loss: 0.15006044507026672 -- PDE Loss: 71.36961364746094  -- Error: 0.40810224413871765 -- k: 4.878228664398193\n",
      "Epoch: 4400 -- Total Loss: 52.46623992919922  -- Data Loss: 0.13128870725631714 -- PDE Loss: 37.98345184326172  -- Error: 0.42856907844543457 -- k: 4.879964351654053\n",
      "Epoch: 4600 -- Total Loss: 70.21182250976562  -- Data Loss: 0.12285634130239487 -- PDE Loss: 56.07699203491211  -- Error: 0.38287341594696045 -- k: 4.881839275360107\n",
      "Epoch: 4800 -- Total Loss: 52.676719665527344  -- Data Loss: 0.1487579643726349 -- PDE Loss: 38.23198699951172  -- Error: 0.389844685792923 -- k: 4.883992671966553\n",
      "Training at resolution: 400x400\n",
      "Epoch: 0 -- Total Loss: 72.47015380859375  -- Data Loss: 0.12062874436378479 -- PDE Loss: 58.772613525390625  -- Error: 0.38866472244262695 -- k: 4.885863304138184\n",
      "Epoch: 200 -- Total Loss: 71.53578186035156  -- Data Loss: 0.12603497505187988 -- PDE Loss: 57.96908950805664  -- Error: 0.404955118894577 -- k: 4.888336181640625\n",
      "Epoch: 400 -- Total Loss: 44.37446975708008  -- Data Loss: 0.09686637669801712 -- PDE Loss: 31.649293899536133  -- Error: 0.340666264295578 -- k: 4.890946865081787\n",
      "Epoch: 600 -- Total Loss: 57.214290618896484  -- Data Loss: 0.11952926218509674 -- PDE Loss: 44.34215545654297  -- Error: 0.3632841110229492 -- k: 4.894060134887695\n",
      "Epoch: 800 -- Total Loss: 80.68087768554688  -- Data Loss: 0.09870581328868866 -- PDE Loss: 68.51104736328125  -- Error: 0.4240744113922119 -- k: 4.897002696990967\n",
      "Epoch: 1000 -- Total Loss: 41.775997161865234  -- Data Loss: 0.09646545350551605 -- PDE Loss: 29.90976333618164  -- Error: 0.34714022278785706 -- k: 4.899615287780762\n",
      "Epoch: 1200 -- Total Loss: 56.43170166015625  -- Data Loss: 0.11411821097135544 -- PDE Loss: 44.47423553466797  -- Error: 0.34916266798973083 -- k: 4.902308940887451\n",
      "Epoch: 1400 -- Total Loss: 46.69221496582031  -- Data Loss: 0.09504460543394089 -- PDE Loss: 35.421417236328125  -- Error: 0.3187900185585022 -- k: 4.905418395996094\n",
      "Epoch: 1600 -- Total Loss: 71.503173828125  -- Data Loss: 0.08026105910539627 -- PDE Loss: 60.83159637451172  -- Error: 0.29708024859428406 -- k: 4.908496379852295\n",
      "Epoch: 1800 -- Total Loss: 42.58013153076172  -- Data Loss: 0.08209791779518127 -- PDE Loss: 32.144187927246094  -- Error: 0.29330915212631226 -- k: 4.911314487457275\n",
      "Epoch: 2000 -- Total Loss: 41.43054962158203  -- Data Loss: 0.07333193719387054 -- PDE Loss: 31.43224334716797  -- Error: 0.32295963168144226 -- k: 4.913922309875488\n",
      "Epoch: 2200 -- Total Loss: 54.09608840942383  -- Data Loss: 0.08656367659568787 -- PDE Loss: 44.10187530517578  -- Error: 0.30758926272392273 -- k: 4.916717529296875\n",
      "Epoch: 2400 -- Total Loss: 32.582271575927734  -- Data Loss: 0.07300733774900436 -- PDE Loss: 23.14799690246582  -- Error: 0.29961150884628296 -- k: 4.9196343421936035\n",
      "Epoch: 2600 -- Total Loss: 30.189844131469727  -- Data Loss: 0.07365849614143372 -- PDE Loss: 21.021724700927734  -- Error: 0.280352383852005 -- k: 4.922451019287109\n",
      "Epoch: 2800 -- Total Loss: 47.237098693847656  -- Data Loss: 0.07239356637001038 -- PDE Loss: 38.38905715942383  -- Error: 0.28185805678367615 -- k: 4.925492286682129\n",
      "Epoch: 3000 -- Total Loss: 38.4119873046875  -- Data Loss: 0.07400546967983246 -- PDE Loss: 29.836084365844727  -- Error: 0.2706966996192932 -- k: 4.928539276123047\n",
      "Epoch: 3200 -- Total Loss: 42.248291015625  -- Data Loss: 0.06988363713026047 -- PDE Loss: 34.07659149169922  -- Error: 0.31716054677963257 -- k: 4.931828022003174\n",
      "Epoch: 3400 -- Total Loss: 52.03329086303711  -- Data Loss: 0.06960901618003845 -- PDE Loss: 44.16484069824219  -- Error: 0.2942121624946594 -- k: 4.934818267822266\n",
      "Epoch: 3600 -- Total Loss: 34.906211853027344  -- Data Loss: 0.06565003097057343 -- PDE Loss: 27.428071975708008  -- Error: 0.22749722003936768 -- k: 4.937959671020508\n",
      "Epoch: 3800 -- Total Loss: 44.96025085449219  -- Data Loss: 0.06043339893221855 -- PDE Loss: 37.8814697265625  -- Error: 0.23716634511947632 -- k: 4.940984725952148\n",
      "Epoch: 4000 -- Total Loss: 23.7957706451416  -- Data Loss: 0.05719951167702675 -- PDE Loss: 17.03786277770996  -- Error: 0.22519730031490326 -- k: 4.943538665771484\n",
      "Epoch: 4200 -- Total Loss: 42.382076263427734  -- Data Loss: 0.04898521304130554 -- PDE Loss: 36.10066604614258  -- Error: 0.23190142214298248 -- k: 4.946720600128174\n",
      "Epoch: 4400 -- Total Loss: 42.38047409057617  -- Data Loss: 0.07291879504919052 -- PDE Loss: 35.92019271850586  -- Error: 0.27245640754699707 -- k: 4.949740886688232\n",
      "Epoch: 4600 -- Total Loss: 35.58625411987305  -- Data Loss: 0.05660678446292877 -- PDE Loss: 29.78294563293457  -- Error: 0.2639119029045105 -- k: 4.953125\n",
      "Epoch: 4800 -- Total Loss: 32.313568115234375  -- Data Loss: 0.05051300302147865 -- PDE Loss: 26.975723266601562  -- Error: 0.22795219719409943 -- k: 4.956533432006836\n",
      "Training at resolution: 800x800\n",
      "Epoch: 0 -- Total Loss: 44.19124221801758  -- Data Loss: 0.050033699721097946 -- PDE Loss: 39.1391487121582  -- Error: 0.25346770882606506 -- k: 4.959332466125488\n",
      "Epoch: 200 -- Total Loss: 19.91010284423828  -- Data Loss: 0.03220918029546738 -- PDE Loss: 15.589466094970703  -- Error: 0.1359396129846573 -- k: 4.963110446929932\n",
      "Epoch: 400 -- Total Loss: 14.917901992797852  -- Data Loss: 0.027384208515286446 -- PDE Loss: 11.021905899047852  -- Error: 0.1451612263917923 -- k: 4.966406345367432\n",
      "Epoch: 600 -- Total Loss: 28.807586669921875  -- Data Loss: 0.03794205188751221 -- PDE Loss: 25.083276748657227  -- Error: 0.14636041224002838 -- k: 4.97025728225708\n",
      "Epoch: 800 -- Total Loss: 16.981475830078125  -- Data Loss: 0.03335630148649216 -- PDE Loss: 13.762490272521973  -- Error: 0.12843738496303558 -- k: 4.974442958831787\n",
      "Epoch: 1000 -- Total Loss: 18.787574768066406  -- Data Loss: 0.030601082369685173 -- PDE Loss: 16.09041976928711  -- Error: 0.14563073217868805 -- k: 4.979136943817139\n",
      "Epoch: 1200 -- Total Loss: 15.189923286437988  -- Data Loss: 0.029068510979413986 -- PDE Loss: 12.89018440246582  -- Error: 0.11206357926130295 -- k: 4.982791423797607\n",
      "Epoch: 1400 -- Total Loss: 29.531545639038086  -- Data Loss: 0.02939857915043831 -- PDE Loss: 27.70698356628418  -- Error: 0.21911080181598663 -- k: 4.987644195556641\n",
      "Epoch: 1600 -- Total Loss: 22.672550201416016  -- Data Loss: 0.023447882384061813 -- PDE Loss: 21.426233291625977  -- Error: 0.13750913739204407 -- k: 4.992265224456787\n",
      "Epoch: 1800 -- Total Loss: 11.11946964263916  -- Data Loss: 0.024505924433469772 -- PDE Loss: 10.20508098602295  -- Error: 0.09578048437833786 -- k: 4.995764255523682\n",
      "Epoch: 2000 -- Total Loss: 16.641830444335938  -- Data Loss: 0.028325777500867844 -- PDE Loss: 15.994099617004395  -- Error: 0.15804079174995422 -- k: 4.999207496643066\n",
      "Epoch: 2200 -- Total Loss: 13.52330207824707  -- Data Loss: 0.01588066853582859 -- PDE Loss: 13.203800201416016  -- Error: 0.09981632977724075 -- k: 4.999972343444824\n",
      "Epoch: 2400 -- Total Loss: 28.978408813476562  -- Data Loss: 0.020172247663140297 -- PDE Loss: 28.57120704650879  -- Error: 0.1397961527109146 -- k: 4.999987602233887\n",
      "Epoch: 2600 -- Total Loss: 23.116043090820312  -- Data Loss: 0.02426290325820446 -- PDE Loss: 22.630441665649414  -- Error: 0.10757304728031158 -- k: 5.000002384185791\n",
      "Epoch: 2800 -- Total Loss: 27.03230094909668  -- Data Loss: 0.024316100403666496 -- PDE Loss: 26.54515838623047  -- Error: 0.1371232569217682 -- k: 5.000056743621826\n",
      "Epoch: 3000 -- Total Loss: 32.55559539794922  -- Data Loss: 0.01875116676092148 -- PDE Loss: 32.18024826049805  -- Error: 0.11139523237943649 -- k: 5.000025272369385\n",
      "Epoch: 3200 -- Total Loss: 18.913835525512695  -- Data Loss: 0.02543100342154503 -- PDE Loss: 18.40374755859375  -- Error: 0.11127737164497375 -- k: 4.9999895095825195\n",
      "Epoch: 3400 -- Total Loss: 19.71782684326172  -- Data Loss: 0.024700643494725227 -- PDE Loss: 19.217844009399414  -- Error: 0.11216189712285995 -- k: 5.000037670135498\n",
      "Epoch: 3600 -- Total Loss: 15.502684593200684  -- Data Loss: 0.02273929864168167 -- PDE Loss: 15.04345417022705  -- Error: 0.10818729549646378 -- k: 5.0000386238098145\n",
      "Epoch: 3800 -- Total Loss: 21.844175338745117  -- Data Loss: 0.018312111496925354 -- PDE Loss: 21.473299026489258  -- Error: 0.09982194006443024 -- k: 5.000070571899414\n",
      "Epoch: 4000 -- Total Loss: 11.065400123596191  -- Data Loss: 0.018583081662654877 -- PDE Loss: 10.689447402954102  -- Error: 0.07941054552793503 -- k: 5.000035762786865\n",
      "Epoch: 4200 -- Total Loss: 8.447651863098145  -- Data Loss: 0.01577659510076046 -- PDE Loss: 8.130632400512695  -- Error: 0.08069206029176712 -- k: 4.999996662139893\n",
      "Epoch: 4400 -- Total Loss: 18.809595108032227  -- Data Loss: 0.01790345273911953 -- PDE Loss: 18.44605255126953  -- Error: 0.11901882290840149 -- k: 5.000025749206543\n",
      "Epoch: 4600 -- Total Loss: 12.54345989227295  -- Data Loss: 0.019126372411847115 -- PDE Loss: 12.160703659057617  -- Error: 0.092205710709095 -- k: 4.99999475479126\n",
      "Epoch: 4800 -- Total Loss: 18.143999099731445  -- Data Loss: 0.017857566475868225 -- PDE Loss: 17.78272819519043  -- Error: 0.091208815574646 -- k: 5.000052452087402\n",
      "Training at resolution: 1000x1000\n",
      "Epoch: 0 -- Total Loss: 7.55682373046875  -- Data Loss: 0.021008560433983803 -- PDE Loss: 7.134497165679932  -- Error: 0.10313406586647034 -- k: 5.000024795532227\n",
      "Epoch: 200 -- Total Loss: 15.558579444885254  -- Data Loss: 0.014222967438399792 -- PDE Loss: 15.273414611816406  -- Error: 0.1101405918598175 -- k: 5.0\n",
      "Epoch: 400 -- Total Loss: 32.33942794799805  -- Data Loss: 0.02399679459631443 -- PDE Loss: 31.849153518676758  -- Error: 0.17650017142295837 -- k: 5.000127792358398\n",
      "Epoch: 600 -- Total Loss: 7.703709602355957  -- Data Loss: 0.013297980651259422 -- PDE Loss: 7.43153190612793  -- Error: 0.08811197429895401 -- k: 5.000036716461182\n",
      "Epoch: 800 -- Total Loss: 21.71533966064453  -- Data Loss: 0.018189100548624992 -- PDE Loss: 21.347665786743164  -- Error: 0.11150247603654861 -- k: 4.999954700469971\n",
      "Epoch: 1000 -- Total Loss: 22.53330421447754  -- Data Loss: 0.0239997748285532 -- PDE Loss: 22.05155372619629  -- Error: 0.12050973623991013 -- k: 4.999965667724609\n",
      "Epoch: 1200 -- Total Loss: 7.146528244018555  -- Data Loss: 0.014422153122723103 -- PDE Loss: 6.856635570526123  -- Error: 0.08263023942708969 -- k: 4.999977111816406\n",
      "Epoch: 1400 -- Total Loss: 21.691356658935547  -- Data Loss: 0.0181146040558815 -- PDE Loss: 21.31886100769043  -- Error: 0.11194166541099548 -- k: 4.999900817871094\n",
      "Epoch: 1600 -- Total Loss: 9.413722038269043  -- Data Loss: 0.01996200904250145 -- PDE Loss: 9.009961128234863  -- Error: 0.13147403299808502 -- k: 4.999950408935547\n",
      "Epoch: 1800 -- Total Loss: 9.91224193572998  -- Data Loss: 0.015939883887767792 -- PDE Loss: 9.584040641784668  -- Error: 0.1041887179017067 -- k: 4.999924659729004\n",
      "Epoch: 2000 -- Total Loss: 7.903994083404541  -- Data Loss: 0.014040110632777214 -- PDE Loss: 7.622886657714844  -- Error: 0.07542905956506729 -- k: 4.999988079071045\n",
      "Epoch: 2200 -- Total Loss: 11.903130531311035  -- Data Loss: 0.01405129674822092 -- PDE Loss: 11.617984771728516  -- Error: 0.08448037505149841 -- k: 4.9999589920043945\n",
      "Epoch: 2400 -- Total Loss: 11.84344482421875  -- Data Loss: 0.01185033842921257 -- PDE Loss: 11.602832794189453  -- Error: 0.08082621544599533 -- k: 5.000044345855713\n",
      "Epoch: 2600 -- Total Loss: 9.199333190917969  -- Data Loss: 0.017420144751667976 -- PDE Loss: 8.848526954650879  -- Error: 0.07741773128509521 -- k: 4.999969959259033\n",
      "Epoch: 2800 -- Total Loss: 17.362396240234375  -- Data Loss: 0.017241530120372772 -- PDE Loss: 17.00942039489746  -- Error: 0.1104351058602333 -- k: 4.99992561340332\n",
      "Epoch: 3000 -- Total Loss: 15.137137413024902  -- Data Loss: 0.021941842511296272 -- PDE Loss: 14.697861671447754  -- Error: 0.12228702008724213 -- k: 4.999964237213135\n",
      "Epoch: 3200 -- Total Loss: 14.01124382019043  -- Data Loss: 0.016113894060254097 -- PDE Loss: 13.688164710998535  -- Error: 0.08095179498195648 -- k: 4.99998664855957\n",
      "Epoch: 3400 -- Total Loss: 9.892098426818848  -- Data Loss: 0.011723781935870647 -- PDE Loss: 9.656363487243652  -- Error: 0.07894553989171982 -- k: 4.999996185302734\n",
      "Epoch: 3600 -- Total Loss: 10.195647239685059  -- Data Loss: 0.010744164697825909 -- PDE Loss: 9.97431755065918  -- Error: 0.09620832651853561 -- k: 5.0000529289245605\n",
      "Epoch: 3800 -- Total Loss: 14.304231643676758  -- Data Loss: 0.014239824377000332 -- PDE Loss: 14.01489543914795  -- Error: 0.10282058268785477 -- k: 4.999954700469971\n",
      "Epoch: 4000 -- Total Loss: 19.777080535888672  -- Data Loss: 0.016329843550920486 -- PDE Loss: 19.44685935974121  -- Error: 0.1394808441400528 -- k: 5.000032424926758\n",
      "Epoch: 4200 -- Total Loss: 11.181282043457031  -- Data Loss: 0.012114958837628365 -- PDE Loss: 10.937361717224121  -- Error: 0.09915737062692642 -- k: 5.0000200271606445\n",
      "Epoch: 4400 -- Total Loss: 7.911914825439453  -- Data Loss: 0.009102421812713146 -- PDE Loss: 7.728455066680908  -- Error: 0.08636198937892914 -- k: 4.999974727630615\n",
      "Epoch: 4600 -- Total Loss: 8.367101669311523  -- Data Loss: 0.01140732690691948 -- PDE Loss: 8.131688117980957  -- Error: 0.09479252249002457 -- k: 5.000066757202148\n",
      "Epoch: 4800 -- Total Loss: 9.868219375610352  -- Data Loss: 0.014702492393553257 -- PDE Loss: 9.570850372314453  -- Error: 0.08806238323450089 -- k: 5.000034332275391\n",
      "Training at resolution: 1200x1200\n",
      "Epoch: 0 -- Total Loss: 11.228767395019531  -- Data Loss: 0.01611187495291233 -- PDE Loss: 10.903286933898926  -- Error: 0.15722087025642395 -- k: 4.999946594238281\n",
      "Epoch: 200 -- Total Loss: 14.823698043823242  -- Data Loss: 0.0173224788159132 -- PDE Loss: 14.475150108337402  -- Error: 0.10457112640142441 -- k: 5.000005722045898\n",
      "Epoch: 400 -- Total Loss: 15.2673921585083  -- Data Loss: 0.013591068796813488 -- PDE Loss: 14.989372253417969  -- Error: 0.0887223333120346 -- k: 4.999944686889648\n",
      "Epoch: 600 -- Total Loss: 6.9289984703063965  -- Data Loss: 0.013707070611417294 -- PDE Loss: 6.654513835906982  -- Error: 0.08085948973894119 -- k: 5.000014305114746\n",
      "Epoch: 800 -- Total Loss: 10.912093162536621  -- Data Loss: 0.010505249723792076 -- PDE Loss: 10.700653076171875  -- Error: 0.05885781720280647 -- k: 5.000020503997803\n",
      "Epoch: 1000 -- Total Loss: 10.603638648986816  -- Data Loss: 0.008034130558371544 -- PDE Loss: 10.438244819641113  -- Error: 0.06820623576641083 -- k: 4.9999542236328125\n",
      "Epoch: 1200 -- Total Loss: 22.852563858032227  -- Data Loss: 0.02287963777780533 -- PDE Loss: 22.393999099731445  -- Error: 0.1700737029314041 -- k: 5.000007152557373\n",
      "Epoch: 1400 -- Total Loss: 21.239913940429688  -- Data Loss: 0.018508512526750565 -- PDE Loss: 20.863792419433594  -- Error: 0.2046222984790802 -- k: 4.999962329864502\n",
      "Epoch: 1600 -- Total Loss: 13.59284782409668  -- Data Loss: 0.014730026945471764 -- PDE Loss: 13.297884941101074  -- Error: 0.11143860965967178 -- k: 5.000016212463379\n",
      "Epoch: 1800 -- Total Loss: 9.445806503295898  -- Data Loss: 0.011711565777659416 -- PDE Loss: 9.209115028381348  -- Error: 0.0864659771323204 -- k: 5.0000224113464355\n",
      "Epoch: 2000 -- Total Loss: 7.153214931488037  -- Data Loss: 0.012308027595281601 -- PDE Loss: 6.906825542449951  -- Error: 0.05836568400263786 -- k: 4.999985694885254\n",
      "Epoch: 2200 -- Total Loss: 5.979194164276123  -- Data Loss: 0.006836629938334227 -- PDE Loss: 5.8422136306762695  -- Error: 0.0595046728849411 -- k: 5.000006675720215\n",
      "Epoch: 2400 -- Total Loss: 9.221298217773438  -- Data Loss: 0.00889190100133419 -- PDE Loss: 9.041686058044434  -- Error: 0.0919877216219902 -- k: 5.000010967254639\n",
      "Epoch: 2600 -- Total Loss: 14.883829116821289  -- Data Loss: 0.01032322458922863 -- PDE Loss: 14.672042846679688  -- Error: 0.0840298980474472 -- k: 5.00002384185791\n",
      "Epoch: 2800 -- Total Loss: 11.660829544067383  -- Data Loss: 0.011801164597272873 -- PDE Loss: 11.419809341430664  -- Error: 0.08074205368757248 -- k: 5.0000386238098145\n",
      "Epoch: 3000 -- Total Loss: 7.184391498565674  -- Data Loss: 0.011490164324641228 -- PDE Loss: 6.953100681304932  -- Error: 0.07566601037979126 -- k: 4.999997615814209\n",
      "Epoch: 3200 -- Total Loss: 5.2723822593688965  -- Data Loss: 0.007616846822202206 -- PDE Loss: 5.117298603057861  -- Error: 0.04842708259820938 -- k: 5.000017166137695\n",
      "Epoch: 3400 -- Total Loss: 10.524876594543457  -- Data Loss: 0.006312081124633551 -- PDE Loss: 10.397833824157715  -- Error: 0.08345305174589157 -- k: 5.000003814697266\n",
      "Epoch: 3600 -- Total Loss: 8.51450252532959  -- Data Loss: 0.006428454536944628 -- PDE Loss: 8.383454322814941  -- Error: 0.07712197303771973 -- k: 5.000019550323486\n",
      "Epoch: 3800 -- Total Loss: 10.919548988342285  -- Data Loss: 0.013525225222110748 -- PDE Loss: 10.648624420166016  -- Error: 0.16384148597717285 -- k: 4.999999523162842\n",
      "Epoch: 4000 -- Total Loss: 9.67523193359375  -- Data Loss: 0.011919084936380386 -- PDE Loss: 9.43631649017334  -- Error: 0.0866665244102478 -- k: 4.999980449676514\n",
      "Epoch: 4200 -- Total Loss: 11.483855247497559  -- Data Loss: 0.017583174630999565 -- PDE Loss: 11.12984561920166  -- Error: 0.12438745051622391 -- k: 4.999993801116943\n",
      "Epoch: 4400 -- Total Loss: 6.778590679168701  -- Data Loss: 0.01367501076310873 -- PDE Loss: 6.504727840423584  -- Error: 0.07949423044919968 -- k: 5.000003337860107\n",
      "Epoch: 4600 -- Total Loss: 7.076782703399658  -- Data Loss: 0.014295201748609543 -- PDE Loss: 6.7908406257629395  -- Error: 0.04708047956228256 -- k: 5.00000524520874\n",
      "Epoch: 4800 -- Total Loss: 4.740792751312256  -- Data Loss: 0.0057303765788674355 -- PDE Loss: 4.625079154968262  -- Error: 0.05194907635450363 -- k: 5.000005722045898\n",
      "time:2:00:28 error: 0.02008608914911747\n",
      "Training at resolution: 100x100\n",
      "Epoch: 0 -- Total Loss: 271634.71875  -- Data Loss: 1.8334821462631226 -- PDE Loss: 271584.15625  -- Error: 2.129631519317627 -- k: 6.900000095367432\n",
      "Epoch: 200 -- Total Loss: 41517.734375  -- Data Loss: 0.07398350536823273 -- PDE Loss: 41502.37890625  -- Error: 0.917048990726471 -- k: 6.9001946449279785\n",
      "Epoch: 400 -- Total Loss: 6718.99755859375  -- Data Loss: 0.2323765903711319 -- PDE Loss: 6701.33056640625  -- Error: 0.5266147255897522 -- k: 6.906399726867676\n",
      "Epoch: 600 -- Total Loss: 3882.97021484375  -- Data Loss: 0.3450159430503845 -- PDE Loss: 3863.346435546875  -- Error: 0.5421767830848694 -- k: 6.9085187911987305\n",
      "Epoch: 800 -- Total Loss: 3289.455810546875  -- Data Loss: 0.37706756591796875 -- PDE Loss: 3269.26611328125  -- Error: 0.5090675354003906 -- k: 6.909056663513184\n",
      "Epoch: 1000 -- Total Loss: 1517.005126953125  -- Data Loss: 0.3916199207305908 -- PDE Loss: 1496.5662841796875  -- Error: 0.4938633441925049 -- k: 6.909379005432129\n",
      "Epoch: 1200 -- Total Loss: 1551.3802490234375  -- Data Loss: 0.3842279016971588 -- PDE Loss: 1531.102783203125  -- Error: 0.49727728962898254 -- k: 6.909462928771973\n",
      "Epoch: 1400 -- Total Loss: 1209.72802734375  -- Data Loss: 0.39605653285980225 -- PDE Loss: 1189.2342529296875  -- Error: 0.4912906587123871 -- k: 6.909609794616699\n",
      "Epoch: 1600 -- Total Loss: 1290.4549560546875  -- Data Loss: 0.35753607749938965 -- PDE Loss: 1270.7935791015625  -- Error: 0.47915980219841003 -- k: 6.910078048706055\n",
      "Epoch: 1800 -- Total Loss: 1416.7513427734375  -- Data Loss: 0.3941488265991211 -- PDE Loss: 1396.3182373046875  -- Error: 0.5667218565940857 -- k: 6.909745693206787\n",
      "Epoch: 2000 -- Total Loss: 1150.5711669921875  -- Data Loss: 0.32223090529441833 -- PDE Loss: 1131.593994140625  -- Error: 0.468479186296463 -- k: 6.909935474395752\n",
      "Epoch: 2200 -- Total Loss: 1044.537109375  -- Data Loss: 0.31940409541130066 -- PDE Loss: 1025.632080078125  -- Error: 0.5313917398452759 -- k: 6.910018444061279\n",
      "Epoch: 2400 -- Total Loss: 1267.5877685546875  -- Data Loss: 0.30606746673583984 -- PDE Loss: 1248.9525146484375  -- Error: 0.47145357728004456 -- k: 6.910012722015381\n",
      "Epoch: 2600 -- Total Loss: 1228.7481689453125  -- Data Loss: 0.34979909658432007 -- PDE Loss: 1209.198974609375  -- Error: 0.5221662521362305 -- k: 6.909759998321533\n",
      "Epoch: 2800 -- Total Loss: 798.27099609375  -- Data Loss: 0.3195516765117645 -- PDE Loss: 779.3544921875  -- Error: 0.47445347905158997 -- k: 6.909959316253662\n",
      "Epoch: 3000 -- Total Loss: 746.37109375  -- Data Loss: 0.3779352009296417 -- PDE Loss: 726.2871704101562  -- Error: 0.48596087098121643 -- k: 6.909951210021973\n",
      "Epoch: 3200 -- Total Loss: 777.4290771484375  -- Data Loss: 0.3257993459701538 -- PDE Loss: 758.3674926757812  -- Error: 0.4548702836036682 -- k: 6.909815788269043\n",
      "Epoch: 3400 -- Total Loss: 674.654296875  -- Data Loss: 0.3509911596775055 -- PDE Loss: 655.1000366210938  -- Error: 0.49228715896606445 -- k: 6.909864902496338\n",
      "Epoch: 3600 -- Total Loss: 893.6227416992188  -- Data Loss: 0.33508649468421936 -- PDE Loss: 874.3765258789062  -- Error: 0.5142920613288879 -- k: 6.909807205200195\n",
      "Epoch: 3800 -- Total Loss: 551.8320922851562  -- Data Loss: 0.3422434329986572 -- PDE Loss: 532.4415283203125  -- Error: 0.4836125671863556 -- k: 6.9098100662231445\n",
      "Epoch: 4000 -- Total Loss: 554.6140747070312  -- Data Loss: 0.3431682586669922 -- PDE Loss: 535.1975708007812  -- Error: 0.5034656524658203 -- k: 6.909750938415527\n",
      "Epoch: 4200 -- Total Loss: 507.1649169921875  -- Data Loss: 0.3520146310329437 -- PDE Loss: 487.59002685546875  -- Error: 0.4281158745288849 -- k: 6.909890651702881\n",
      "Epoch: 4400 -- Total Loss: 610.998779296875  -- Data Loss: 0.3338511288166046 -- PDE Loss: 591.8010864257812  -- Error: 0.47720521688461304 -- k: 6.909981727600098\n",
      "Epoch: 4600 -- Total Loss: 514.4495239257812  -- Data Loss: 0.3428250849246979 -- PDE Loss: 495.08013916015625  -- Error: 0.4590487778186798 -- k: 6.910018444061279\n",
      "Epoch: 4800 -- Total Loss: 456.6322021484375  -- Data Loss: 0.331033855676651 -- PDE Loss: 437.5340270996094  -- Error: 0.460099995136261 -- k: 6.910274982452393\n",
      "Training at resolution: 200x200\n",
      "Epoch: 0 -- Total Loss: 575.8469848632812  -- Data Loss: 0.2869192659854889 -- PDE Loss: 557.6715087890625  -- Error: 0.43696725368499756 -- k: 6.910607814788818\n",
      "Epoch: 200 -- Total Loss: 361.85357666015625  -- Data Loss: 0.28307101130485535 -- PDE Loss: 343.7579650878906  -- Error: 0.43907299637794495 -- k: 6.910616874694824\n",
      "Epoch: 400 -- Total Loss: 330.54156494140625  -- Data Loss: 0.2701888978481293 -- PDE Loss: 312.7320251464844  -- Error: 0.42303961515426636 -- k: 6.910848617553711\n",
      "Epoch: 600 -- Total Loss: 405.08270263671875  -- Data Loss: 0.2999141216278076 -- PDE Loss: 386.7335510253906  -- Error: 0.4558659791946411 -- k: 6.911195278167725\n",
      "Epoch: 800 -- Total Loss: 446.33721923828125  -- Data Loss: 0.26614806056022644 -- PDE Loss: 428.7110290527344  -- Error: 0.4108816385269165 -- k: 6.911546230316162\n",
      "Epoch: 1000 -- Total Loss: 291.5987243652344  -- Data Loss: 0.2837570011615753 -- PDE Loss: 273.68603515625  -- Error: 0.4669165313243866 -- k: 6.91204309463501\n",
      "Epoch: 1200 -- Total Loss: 431.8101806640625  -- Data Loss: 0.28787779808044434 -- PDE Loss: 413.8725280761719  -- Error: 0.4071803092956543 -- k: 6.912448883056641\n",
      "Epoch: 1400 -- Total Loss: 442.1246643066406  -- Data Loss: 0.2627820074558258 -- PDE Loss: 424.7349548339844  -- Error: 0.40585845708847046 -- k: 6.91278076171875\n",
      "Epoch: 1600 -- Total Loss: 352.2373352050781  -- Data Loss: 0.2599509358406067 -- PDE Loss: 334.97442626953125  -- Error: 0.4453985095024109 -- k: 6.913293838500977\n",
      "Epoch: 1800 -- Total Loss: 324.7805480957031  -- Data Loss: 0.2416621893644333 -- PDE Loss: 307.9477844238281  -- Error: 0.3899897634983063 -- k: 6.913751602172852\n",
      "Epoch: 2000 -- Total Loss: 375.86163330078125  -- Data Loss: 0.27237629890441895 -- PDE Loss: 358.4400329589844  -- Error: 0.4359564483165741 -- k: 6.913936614990234\n",
      "Epoch: 2200 -- Total Loss: 246.48544311523438  -- Data Loss: 0.2557259202003479 -- PDE Loss: 229.50762939453125  -- Error: 0.4019765257835388 -- k: 6.914740085601807\n",
      "Epoch: 2400 -- Total Loss: 205.64273071289062  -- Data Loss: 0.250916451215744 -- PDE Loss: 188.8258514404297  -- Error: 0.3992381989955902 -- k: 6.915225028991699\n",
      "Epoch: 2600 -- Total Loss: 349.97613525390625  -- Data Loss: 0.25207751989364624 -- PDE Loss: 333.2058410644531  -- Error: 0.4117351770401001 -- k: 6.915700435638428\n",
      "Epoch: 2800 -- Total Loss: 200.21267700195312  -- Data Loss: 0.24380949139595032 -- PDE Loss: 183.68167114257812  -- Error: 0.4232834279537201 -- k: 6.916250228881836\n",
      "Epoch: 3000 -- Total Loss: 308.0016174316406  -- Data Loss: 0.2658136487007141 -- PDE Loss: 291.1181335449219  -- Error: 0.42333218455314636 -- k: 6.916879177093506\n",
      "Epoch: 3200 -- Total Loss: 382.99932861328125  -- Data Loss: 0.2681417167186737 -- PDE Loss: 366.14068603515625  -- Error: 0.43610960245132446 -- k: 6.9173970222473145\n",
      "Epoch: 3400 -- Total Loss: 260.0108947753906  -- Data Loss: 0.2116347849369049 -- PDE Loss: 244.32650756835938  -- Error: 0.3912679851055145 -- k: 6.917720794677734\n",
      "Epoch: 3600 -- Total Loss: 276.25213623046875  -- Data Loss: 0.2786789536476135 -- PDE Loss: 259.31878662109375  -- Error: 0.4205976128578186 -- k: 6.91838264465332\n",
      "Epoch: 3800 -- Total Loss: 272.78533935546875  -- Data Loss: 0.23956255614757538 -- PDE Loss: 256.7344970703125  -- Error: 0.4516635537147522 -- k: 6.919111251831055\n",
      "Epoch: 4000 -- Total Loss: 269.1715393066406  -- Data Loss: 0.22694174945354462 -- PDE Loss: 253.43836975097656  -- Error: 0.3838355839252472 -- k: 6.919607162475586\n",
      "Epoch: 4200 -- Total Loss: 389.26416015625  -- Data Loss: 0.24357450008392334 -- PDE Loss: 373.30133056640625  -- Error: 0.4042080044746399 -- k: 6.920325756072998\n",
      "Epoch: 4400 -- Total Loss: 189.38726806640625  -- Data Loss: 0.20844602584838867 -- PDE Loss: 174.19422912597656  -- Error: 0.3989364504814148 -- k: 6.9208221435546875\n",
      "Epoch: 4600 -- Total Loss: 215.11566162109375  -- Data Loss: 0.23153768479824066 -- PDE Loss: 199.59298706054688  -- Error: 0.39795100688934326 -- k: 6.921762466430664\n",
      "Epoch: 4800 -- Total Loss: 215.00509643554688  -- Data Loss: 0.23025599122047424 -- PDE Loss: 199.5618133544922  -- Error: 0.3802592158317566 -- k: 6.922135353088379\n",
      "Training at resolution: 400x400\n",
      "Epoch: 0 -- Total Loss: 230.18495178222656  -- Data Loss: 0.2214181125164032 -- PDE Loss: 214.99998474121094  -- Error: 0.37663623690605164 -- k: 6.922734260559082\n",
      "Epoch: 200 -- Total Loss: 152.80917358398438  -- Data Loss: 0.2287353277206421 -- PDE Loss: 137.61843872070312  -- Error: 0.3881453275680542 -- k: 6.923755168914795\n",
      "Epoch: 400 -- Total Loss: 231.3772735595703  -- Data Loss: 0.24060457944869995 -- PDE Loss: 216.0824737548828  -- Error: 0.4000808298587799 -- k: 6.924745082855225\n",
      "Epoch: 600 -- Total Loss: 175.80227661132812  -- Data Loss: 0.1837691068649292 -- PDE Loss: 161.73973083496094  -- Error: 0.33245226740837097 -- k: 6.925422668457031\n",
      "Epoch: 800 -- Total Loss: 212.2567138671875  -- Data Loss: 0.22120822966098785 -- PDE Loss: 197.5805206298828  -- Error: 0.36011576652526855 -- k: 6.926394939422607\n",
      "Epoch: 1000 -- Total Loss: 188.10946655273438  -- Data Loss: 0.2017631232738495 -- PDE Loss: 173.9463653564453  -- Error: 0.3320253789424896 -- k: 6.927309036254883\n",
      "Epoch: 1200 -- Total Loss: 252.16949462890625  -- Data Loss: 0.1951121687889099 -- PDE Loss: 238.24647521972656  -- Error: 0.41007205843925476 -- k: 6.9280619621276855\n",
      "Epoch: 1400 -- Total Loss: 164.71054077148438  -- Data Loss: 0.18332824110984802 -- PDE Loss: 151.15597534179688  -- Error: 0.32638269662857056 -- k: 6.929030895233154\n",
      "Epoch: 1600 -- Total Loss: 167.57232666015625  -- Data Loss: 0.18251189589500427 -- PDE Loss: 154.1528778076172  -- Error: 0.3518856167793274 -- k: 6.929872512817383\n",
      "Epoch: 1800 -- Total Loss: 121.79283905029297  -- Data Loss: 0.17202939093112946 -- PDE Loss: 108.71182250976562  -- Error: 0.31287112832069397 -- k: 6.930816650390625\n",
      "Epoch: 2000 -- Total Loss: 135.17848205566406  -- Data Loss: 0.16947117447853088 -- PDE Loss: 122.26093292236328  -- Error: 0.34328269958496094 -- k: 6.931605815887451\n",
      "Epoch: 2200 -- Total Loss: 157.12362670898438  -- Data Loss: 0.17568837106227875 -- PDE Loss: 144.1982421875  -- Error: 0.31448331475257874 -- k: 6.932441711425781\n",
      "Epoch: 2400 -- Total Loss: 98.66820526123047  -- Data Loss: 0.16181449592113495 -- PDE Loss: 86.12248992919922  -- Error: 0.33728736639022827 -- k: 6.933186054229736\n",
      "Epoch: 2600 -- Total Loss: 121.97459411621094  -- Data Loss: 0.17181164026260376 -- PDE Loss: 109.35169219970703  -- Error: 0.3067036271095276 -- k: 6.934086322784424\n",
      "Epoch: 2800 -- Total Loss: 129.57479858398438  -- Data Loss: 0.17028066515922546 -- PDE Loss: 117.08493041992188  -- Error: 0.3251384198665619 -- k: 6.934829235076904\n",
      "Epoch: 3000 -- Total Loss: 156.73391723632812  -- Data Loss: 0.18215496838092804 -- PDE Loss: 144.138671875  -- Error: 0.31633636355400085 -- k: 6.935756206512451\n",
      "Epoch: 3200 -- Total Loss: 129.83877563476562  -- Data Loss: 0.16445070505142212 -- PDE Loss: 117.73989868164062  -- Error: 0.3171539604663849 -- k: 6.936794281005859\n",
      "Epoch: 3400 -- Total Loss: 132.3590087890625  -- Data Loss: 0.18229982256889343 -- PDE Loss: 120.0416030883789  -- Error: 0.3200905919075012 -- k: 6.937787055969238\n",
      "Epoch: 3600 -- Total Loss: 107.2553482055664  -- Data Loss: 0.16066449880599976 -- PDE Loss: 95.49763488769531  -- Error: 0.28546157479286194 -- k: 6.93870735168457\n",
      "Epoch: 3800 -- Total Loss: 124.40858459472656  -- Data Loss: 0.1577940136194229 -- PDE Loss: 112.83660125732422  -- Error: 0.3007383644580841 -- k: 6.939630031585693\n",
      "Epoch: 4000 -- Total Loss: 103.44344329833984  -- Data Loss: 0.17107726633548737 -- PDE Loss: 91.71550750732422  -- Error: 0.30521735548973083 -- k: 6.940402984619141\n",
      "Epoch: 4200 -- Total Loss: 136.90652465820312  -- Data Loss: 0.15274716913700104 -- PDE Loss: 125.68195343017578  -- Error: 0.32908016443252563 -- k: 6.941410541534424\n",
      "Epoch: 4400 -- Total Loss: 102.55558013916016  -- Data Loss: 0.14969296753406525 -- PDE Loss: 91.52423095703125  -- Error: 0.29208648204803467 -- k: 6.9423699378967285\n",
      "Epoch: 4600 -- Total Loss: 129.4873046875  -- Data Loss: 0.16710448265075684 -- PDE Loss: 118.22410583496094  -- Error: 0.35053306818008423 -- k: 6.943185806274414\n",
      "Epoch: 4800 -- Total Loss: 106.47057342529297  -- Data Loss: 0.13475364446640015 -- PDE Loss: 96.00656127929688  -- Error: 0.2612724304199219 -- k: 6.944277763366699\n",
      "Training at resolution: 800x800\n",
      "Epoch: 0 -- Total Loss: 141.76951599121094  -- Data Loss: 0.15899330377578735 -- PDE Loss: 130.97108459472656  -- Error: 0.29462116956710815 -- k: 6.945373058319092\n",
      "Epoch: 200 -- Total Loss: 117.89608001708984  -- Data Loss: 0.13989202678203583 -- PDE Loss: 107.66911315917969  -- Error: 0.296697735786438 -- k: 6.9467339515686035\n",
      "Epoch: 400 -- Total Loss: 80.4969253540039  -- Data Loss: 0.11638204753398895 -- PDE Loss: 70.95679473876953  -- Error: 0.24842418730258942 -- k: 6.948288917541504\n",
      "Epoch: 600 -- Total Loss: 57.057857513427734  -- Data Loss: 0.12469194829463959 -- PDE Loss: 47.509193420410156  -- Error: 0.2515637278556824 -- k: 6.949433326721191\n",
      "Epoch: 800 -- Total Loss: 80.21985626220703  -- Data Loss: 0.12245293706655502 -- PDE Loss: 70.89606475830078  -- Error: 0.25085240602493286 -- k: 6.95072603225708\n",
      "Epoch: 1000 -- Total Loss: 62.086971282958984  -- Data Loss: 0.1071317046880722 -- PDE Loss: 53.26472473144531  -- Error: 0.2271656095981598 -- k: 6.952136516571045\n",
      "Epoch: 1200 -- Total Loss: 95.90977478027344  -- Data Loss: 0.10281171649694443 -- PDE Loss: 87.3569564819336  -- Error: 0.24720169603824615 -- k: 6.953450679779053\n",
      "Epoch: 1400 -- Total Loss: 157.640869140625  -- Data Loss: 0.12620504200458527 -- PDE Loss: 148.818603515625  -- Error: 0.24691106379032135 -- k: 6.954867839813232\n",
      "Epoch: 1600 -- Total Loss: 60.031654357910156  -- Data Loss: 0.1101769506931305 -- PDE Loss: 51.69278335571289  -- Error: 0.24041400849819183 -- k: 6.956047058105469\n",
      "Epoch: 1800 -- Total Loss: 73.37970733642578  -- Data Loss: 0.11535357683897018 -- PDE Loss: 65.14982604980469  -- Error: 0.26624399423599243 -- k: 6.957578182220459\n",
      "Epoch: 2000 -- Total Loss: 54.703086853027344  -- Data Loss: 0.10433389991521835 -- PDE Loss: 46.8466796875  -- Error: 0.22884917259216309 -- k: 6.958669662475586\n",
      "Epoch: 2200 -- Total Loss: 46.649532318115234  -- Data Loss: 0.10143329203128815 -- PDE Loss: 39.00170135498047  -- Error: 0.2176382839679718 -- k: 6.959746360778809\n",
      "Epoch: 2400 -- Total Loss: 78.74310302734375  -- Data Loss: 0.10164621472358704 -- PDE Loss: 71.2792739868164  -- Error: 0.2428976148366928 -- k: 6.961110591888428\n",
      "Epoch: 2600 -- Total Loss: 93.95631408691406  -- Data Loss: 0.1013442724943161 -- PDE Loss: 86.66411590576172  -- Error: 0.24934999644756317 -- k: 6.962303161621094\n",
      "Epoch: 2800 -- Total Loss: 44.04197311401367  -- Data Loss: 0.09656663984060287 -- PDE Loss: 37.00978088378906  -- Error: 0.22083477675914764 -- k: 6.963477611541748\n",
      "Epoch: 3000 -- Total Loss: 97.54324340820312  -- Data Loss: 0.10999329388141632 -- PDE Loss: 90.40109252929688  -- Error: 0.32940778136253357 -- k: 6.964632511138916\n",
      "Epoch: 3200 -- Total Loss: 50.289695739746094  -- Data Loss: 0.09473645687103271 -- PDE Loss: 43.59184265136719  -- Error: 0.20531247556209564 -- k: 6.965622425079346\n",
      "Epoch: 3400 -- Total Loss: 73.96562194824219  -- Data Loss: 0.08580197393894196 -- PDE Loss: 67.63066864013672  -- Error: 0.2398999184370041 -- k: 6.966943264007568\n",
      "Epoch: 3600 -- Total Loss: 61.301055908203125  -- Data Loss: 0.09735191613435745 -- PDE Loss: 54.92530822753906  -- Error: 0.2156686633825302 -- k: 6.968297958374023\n",
      "Epoch: 3800 -- Total Loss: 41.574867248535156  -- Data Loss: 0.08642326295375824 -- PDE Loss: 35.59751510620117  -- Error: 0.21442410349845886 -- k: 6.969589710235596\n",
      "Epoch: 4000 -- Total Loss: 99.64549255371094  -- Data Loss: 0.0825192928314209 -- PDE Loss: 93.90830993652344  -- Error: 0.19028756022453308 -- k: 6.970755577087402\n",
      "Epoch: 4200 -- Total Loss: 77.49129486083984  -- Data Loss: 0.07486577332019806 -- PDE Loss: 72.07758331298828  -- Error: 0.19773906469345093 -- k: 6.971970558166504\n",
      "Epoch: 4400 -- Total Loss: 100.07791900634766  -- Data Loss: 0.08573746681213379 -- PDE Loss: 94.62350463867188  -- Error: 0.19148236513137817 -- k: 6.973267555236816\n",
      "Epoch: 4600 -- Total Loss: 95.06767272949219  -- Data Loss: 0.09063616394996643 -- PDE Loss: 89.69755554199219  -- Error: 0.22403161227703094 -- k: 6.974557399749756\n",
      "Epoch: 4800 -- Total Loss: 118.22563171386719  -- Data Loss: 0.09707538038492203 -- PDE Loss: 112.92337799072266  -- Error: 0.2814417779445648 -- k: 6.975958347320557\n",
      "Training at resolution: 1000x1000\n",
      "Epoch: 0 -- Total Loss: 86.9116439819336  -- Data Loss: 0.08045750856399536 -- PDE Loss: 82.11214447021484  -- Error: 0.20147056877613068 -- k: 6.977180480957031\n",
      "Epoch: 200 -- Total Loss: 49.20952224731445  -- Data Loss: 0.07382870465517044 -- PDE Loss: 44.77159881591797  -- Error: 0.22621922194957733 -- k: 6.978830814361572\n",
      "Epoch: 400 -- Total Loss: 49.08988952636719  -- Data Loss: 0.07014713436365128 -- PDE Loss: 44.91110610961914  -- Error: 0.15140558779239655 -- k: 6.980138778686523\n",
      "Epoch: 600 -- Total Loss: 33.41727828979492  -- Data Loss: 0.06254933774471283 -- PDE Loss: 29.569284439086914  -- Error: 0.15927541255950928 -- k: 6.981430530548096\n",
      "Epoch: 800 -- Total Loss: 58.498172760009766  -- Data Loss: 0.07027735561132431 -- PDE Loss: 54.705387115478516  -- Error: 0.17555493116378784 -- k: 6.982932090759277\n",
      "Epoch: 1000 -- Total Loss: 61.64399337768555  -- Data Loss: 0.07609790563583374 -- PDE Loss: 57.9561653137207  -- Error: 0.22775046527385712 -- k: 6.984521865844727\n",
      "Epoch: 1200 -- Total Loss: 56.21105194091797  -- Data Loss: 0.06391485780477524 -- PDE Loss: 52.96417999267578  -- Error: 0.1626794934272766 -- k: 6.9859232902526855\n",
      "Epoch: 1400 -- Total Loss: 31.932634353637695  -- Data Loss: 0.05524052679538727 -- PDE Loss: 29.037853240966797  -- Error: 0.14312034845352173 -- k: 6.987210273742676\n",
      "Epoch: 1600 -- Total Loss: 41.4425048828125  -- Data Loss: 0.04982982203364372 -- PDE Loss: 38.82496643066406  -- Error: 0.13426990807056427 -- k: 6.988419055938721\n",
      "Epoch: 1800 -- Total Loss: 97.70721435546875  -- Data Loss: 0.05871119722723961 -- PDE Loss: 95.11933898925781  -- Error: 0.1751602441072464 -- k: 6.989887714385986\n",
      "Epoch: 2000 -- Total Loss: 45.582149505615234  -- Data Loss: 0.05851750075817108 -- PDE Loss: 43.17324447631836  -- Error: 0.15357781946659088 -- k: 6.991161823272705\n",
      "Epoch: 2200 -- Total Loss: 65.46133422851562  -- Data Loss: 0.04942041635513306 -- PDE Loss: 63.41651916503906  -- Error: 0.16408075392246246 -- k: 6.992484092712402\n",
      "Epoch: 2400 -- Total Loss: 103.34568786621094  -- Data Loss: 0.08355151116847992 -- PDE Loss: 100.81314849853516  -- Error: 0.3185714781284332 -- k: 6.993834018707275\n",
      "Epoch: 2600 -- Total Loss: 114.99687194824219  -- Data Loss: 0.06103382632136345 -- PDE Loss: 113.0716552734375  -- Error: 0.19718989729881287 -- k: 6.994948387145996\n",
      "Epoch: 2800 -- Total Loss: 60.5120735168457  -- Data Loss: 0.052892714738845825 -- PDE Loss: 58.939273834228516  -- Error: 0.14348524808883667 -- k: 6.996318817138672\n",
      "Epoch: 3000 -- Total Loss: 56.969058990478516  -- Data Loss: 0.046973563730716705 -- PDE Loss: 55.69793701171875  -- Error: 0.14543090760707855 -- k: 6.997640132904053\n",
      "Epoch: 3200 -- Total Loss: 63.728355407714844  -- Data Loss: 0.05464873090386391 -- PDE Loss: 62.49553298950195  -- Error: 0.16634723544120789 -- k: 6.998991012573242\n",
      "Epoch: 3400 -- Total Loss: 46.91689682006836  -- Data Loss: 0.052726030349731445 -- PDE Loss: 45.85917282104492  -- Error: 0.13615131378173828 -- k: 7.000015735626221\n",
      "Epoch: 3600 -- Total Loss: 65.3460464477539  -- Data Loss: 0.04533930867910385 -- PDE Loss: 64.42762756347656  -- Error: 0.14732851088047028 -- k: 7.00007963180542\n",
      "Epoch: 3800 -- Total Loss: 53.34019470214844  -- Data Loss: 0.054634127765893936 -- PDE Loss: 52.241981506347656  -- Error: 0.151402086019516 -- k: 7.000053882598877\n",
      "Epoch: 4000 -- Total Loss: 66.50332641601562  -- Data Loss: 0.052468519657850266 -- PDE Loss: 65.45033264160156  -- Error: 0.16426226496696472 -- k: 7.000027179718018\n",
      "Epoch: 4200 -- Total Loss: 44.46912384033203  -- Data Loss: 0.049264587461948395 -- PDE Loss: 43.47834014892578  -- Error: 0.14916542172431946 -- k: 6.999983787536621\n",
      "Epoch: 4400 -- Total Loss: 56.34883117675781  -- Data Loss: 0.049182724207639694 -- PDE Loss: 55.3565559387207  -- Error: 0.14684386551380157 -- k: 6.999935626983643\n",
      "Epoch: 4600 -- Total Loss: 54.98542785644531  -- Data Loss: 0.046231478452682495 -- PDE Loss: 54.05950164794922  -- Error: 0.20051103830337524 -- k: 7.00000524520874\n",
      "Epoch: 4800 -- Total Loss: 58.643333435058594  -- Data Loss: 0.0507681742310524 -- PDE Loss: 57.62278366088867  -- Error: 0.16343429684638977 -- k: 6.999946594238281\n",
      "Training at resolution: 1200x1200\n",
      "Epoch: 0 -- Total Loss: 55.168704986572266  -- Data Loss: 0.04675109684467316 -- PDE Loss: 54.23147201538086  -- Error: 0.1330547332763672 -- k: 7.000001430511475\n",
      "Epoch: 200 -- Total Loss: 47.83319091796875  -- Data Loss: 0.048213716596364975 -- PDE Loss: 46.865177154541016  -- Error: 0.15903139114379883 -- k: 6.999960899353027\n",
      "Epoch: 400 -- Total Loss: 57.44554138183594  -- Data Loss: 0.05564263090491295 -- PDE Loss: 56.32582092285156  -- Error: 0.14059847593307495 -- k: 7.0000410079956055\n",
      "Epoch: 600 -- Total Loss: 40.679683685302734  -- Data Loss: 0.05449163168668747 -- PDE Loss: 39.57958984375  -- Error: 0.14548084139823914 -- k: 6.99992561340332\n",
      "Epoch: 800 -- Total Loss: 30.155241012573242  -- Data Loss: 0.043108388781547546 -- PDE Loss: 29.290327072143555  -- Error: 0.13394397497177124 -- k: 7.0000200271606445\n",
      "Epoch: 1000 -- Total Loss: 32.319522857666016  -- Data Loss: 0.03899643197655678 -- PDE Loss: 31.53940200805664  -- Error: 0.1143236756324768 -- k: 7.000000476837158\n",
      "Epoch: 1200 -- Total Loss: 23.64918327331543  -- Data Loss: 0.037197962403297424 -- PDE Loss: 22.903392791748047  -- Error: 0.09801574796438217 -- k: 7.000004768371582\n",
      "Epoch: 1400 -- Total Loss: 86.35211181640625  -- Data Loss: 0.04561535641551018 -- PDE Loss: 85.43141174316406  -- Error: 0.22071833908557892 -- k: 7.000048637390137\n",
      "Epoch: 1600 -- Total Loss: 70.65184020996094  -- Data Loss: 0.04532552883028984 -- PDE Loss: 69.73873138427734  -- Error: 0.19536803662776947 -- k: 6.999955177307129\n",
      "Epoch: 1800 -- Total Loss: 30.791095733642578  -- Data Loss: 0.04032036289572716 -- PDE Loss: 29.98285675048828  -- Error: 0.12093430012464523 -- k: 6.9999918937683105\n",
      "Epoch: 2000 -- Total Loss: 27.916784286499023  -- Data Loss: 0.03952560946345329 -- PDE Loss: 27.12245750427246  -- Error: 0.11229239404201508 -- k: 6.999973297119141\n",
      "Epoch: 2200 -- Total Loss: 81.4491195678711  -- Data Loss: 0.046922821551561356 -- PDE Loss: 80.5049819946289  -- Error: 0.2398051768541336 -- k: 7.000062465667725\n",
      "Epoch: 2400 -- Total Loss: 79.65534973144531  -- Data Loss: 0.04583124816417694 -- PDE Loss: 78.73773193359375  -- Error: 0.19104744493961334 -- k: 6.9999871253967285\n",
      "Epoch: 2600 -- Total Loss: 20.95023536682129  -- Data Loss: 0.03969975560903549 -- PDE Loss: 20.15589714050293  -- Error: 0.12141944468021393 -- k: 7.000004768371582\n",
      "Epoch: 2800 -- Total Loss: 38.27672576904297  -- Data Loss: 0.04320913180708885 -- PDE Loss: 37.41132354736328  -- Error: 0.11935070902109146 -- k: 6.999999046325684\n",
      "Epoch: 3000 -- Total Loss: 52.72432327270508  -- Data Loss: 0.039186373353004456 -- PDE Loss: 51.93365478515625  -- Error: 0.13646844029426575 -- k: 7.000046730041504\n",
      "Epoch: 3200 -- Total Loss: 52.02792739868164  -- Data Loss: 0.04254666715860367 -- PDE Loss: 51.176918029785156  -- Error: 0.17036853730678558 -- k: 7.000006675720215\n",
      "Epoch: 3400 -- Total Loss: 36.94304656982422  -- Data Loss: 0.04361310228705406 -- PDE Loss: 36.06861114501953  -- Error: 0.12685371935367584 -- k: 6.999999523162842\n",
      "Epoch: 3600 -- Total Loss: 49.29661178588867  -- Data Loss: 0.035508472472429276 -- PDE Loss: 48.57843017578125  -- Error: 0.12375225126743317 -- k: 7.000044822692871\n",
      "Epoch: 3800 -- Total Loss: 42.584617614746094  -- Data Loss: 0.04060220345854759 -- PDE Loss: 41.76685333251953  -- Error: 0.13122425973415375 -- k: 6.999955654144287\n",
      "Epoch: 4000 -- Total Loss: 35.58041000366211  -- Data Loss: 0.04645096883177757 -- PDE Loss: 34.64723205566406  -- Error: 0.13377442955970764 -- k: 7.000024318695068\n",
      "Epoch: 4200 -- Total Loss: 52.9924430847168  -- Data Loss: 0.041137635707855225 -- PDE Loss: 52.167057037353516  -- Error: 0.14120760560035706 -- k: 7.000009536743164\n",
      "Epoch: 4400 -- Total Loss: 34.326114654541016  -- Data Loss: 0.04087372124195099 -- PDE Loss: 33.50558853149414  -- Error: 0.11695728451013565 -- k: 6.999998569488525\n",
      "Epoch: 4600 -- Total Loss: 15.464845657348633  -- Data Loss: 0.03950778767466545 -- PDE Loss: 14.674156188964844  -- Error: 0.11775495111942291 -- k: 7.0000081062316895\n",
      "Epoch: 4800 -- Total Loss: 61.227081298828125  -- Data Loss: 0.03973429277539253 -- PDE Loss: 60.42686462402344  -- Error: 0.180912047624588 -- k: 7.000044345855713\n",
      "time:1:42:44 error: 0.06680626422166824\n",
      "Training at resolution: 100x100\n",
      "Epoch: 0 -- Total Loss: 370633.5625  -- Data Loss: 1.833646297454834 -- PDE Loss: 370576.96875  -- Error: 2.0993576049804688 -- k: 9.90000057220459\n",
      "Epoch: 200 -- Total Loss: 150853.546875  -- Data Loss: 0.2455407977104187 -- PDE Loss: 150828.65625  -- Error: 0.9888489842414856 -- k: 9.899582862854004\n",
      "Epoch: 400 -- Total Loss: 16247.1826171875  -- Data Loss: 0.7824633121490479 -- PDE Loss: 16211.642578125  -- Error: 0.8308069109916687 -- k: 9.900038719177246\n",
      "Epoch: 600 -- Total Loss: 11141.2978515625  -- Data Loss: 0.7211287617683411 -- PDE Loss: 11106.7333984375  -- Error: 0.8713393807411194 -- k: 9.89876937866211\n",
      "Epoch: 800 -- Total Loss: 11276.4365234375  -- Data Loss: 0.8307936787605286 -- PDE Loss: 11239.390625  -- Error: 0.7296004891395569 -- k: 9.897321701049805\n",
      "Epoch: 1000 -- Total Loss: 8401.271484375  -- Data Loss: 0.7221059203147888 -- PDE Loss: 8366.3359375  -- Error: 0.7059076428413391 -- k: 9.896994590759277\n",
      "Epoch: 1200 -- Total Loss: 6608.25634765625  -- Data Loss: 0.6727597117424011 -- PDE Loss: 6574.15966796875  -- Error: 0.612571120262146 -- k: 9.896252632141113\n",
      "Epoch: 1400 -- Total Loss: 5759.1103515625  -- Data Loss: 0.7972174882888794 -- PDE Loss: 5722.37451171875  -- Error: 0.689602255821228 -- k: 9.895487785339355\n",
      "Epoch: 1600 -- Total Loss: 6093.83935546875  -- Data Loss: 0.6594948768615723 -- PDE Loss: 6059.837890625  -- Error: 0.6274061799049377 -- k: 9.895380973815918\n",
      "Epoch: 1800 -- Total Loss: 5462.529296875  -- Data Loss: 0.7086758017539978 -- PDE Loss: 5427.47998046875  -- Error: 0.6242510676383972 -- k: 9.895059585571289\n",
      "Epoch: 2000 -- Total Loss: 3845.313232421875  -- Data Loss: 0.6421769857406616 -- PDE Loss: 3811.5009765625  -- Error: 0.6008555889129639 -- k: 9.894603729248047\n",
      "Epoch: 2200 -- Total Loss: 3787.650146484375  -- Data Loss: 0.6329948902130127 -- PDE Loss: 3754.008056640625  -- Error: 0.5897197723388672 -- k: 9.894525527954102\n",
      "Epoch: 2400 -- Total Loss: 3589.82861328125  -- Data Loss: 0.6070470809936523 -- PDE Loss: 3556.614013671875  -- Error: 0.5849705338478088 -- k: 9.894065856933594\n",
      "Epoch: 2600 -- Total Loss: 3864.519287109375  -- Data Loss: 0.6765236854553223 -- PDE Loss: 3829.881103515625  -- Error: 0.6678070425987244 -- k: 9.893905639648438\n",
      "Epoch: 2800 -- Total Loss: 2939.46923828125  -- Data Loss: 0.5884385108947754 -- PDE Loss: 2906.505126953125  -- Error: 0.5710936188697815 -- k: 9.893457412719727\n",
      "Epoch: 3000 -- Total Loss: 2535.21533203125  -- Data Loss: 0.6500012278556824 -- PDE Loss: 2500.954833984375  -- Error: 0.5889852046966553 -- k: 9.89311408996582\n",
      "Epoch: 3200 -- Total Loss: 2534.59912109375  -- Data Loss: 0.6231425404548645 -- PDE Loss: 2500.8818359375  -- Error: 0.6305526494979858 -- k: 9.893152236938477\n",
      "Epoch: 3400 -- Total Loss: 2828.231201171875  -- Data Loss: 0.6380388736724854 -- PDE Loss: 2794.166015625  -- Error: 0.6013432145118713 -- k: 9.892877578735352\n",
      "Epoch: 3600 -- Total Loss: 1968.505859375  -- Data Loss: 0.5602277517318726 -- PDE Loss: 1935.9573974609375  -- Error: 0.5273993611335754 -- k: 9.89268684387207\n",
      "Epoch: 3800 -- Total Loss: 3356.15478515625  -- Data Loss: 0.6570318937301636 -- PDE Loss: 3321.665771484375  -- Error: 0.6081253886222839 -- k: 9.892680168151855\n",
      "Epoch: 4000 -- Total Loss: 2648.156494140625  -- Data Loss: 0.5865907073020935 -- PDE Loss: 2615.03466796875  -- Error: 0.5786879658699036 -- k: 9.892468452453613\n",
      "Epoch: 4200 -- Total Loss: 2669.60986328125  -- Data Loss: 0.6158292293548584 -- PDE Loss: 2635.874755859375  -- Error: 0.5820299386978149 -- k: 9.892324447631836\n",
      "Epoch: 4400 -- Total Loss: 2574.57421875  -- Data Loss: 0.5865479707717896 -- PDE Loss: 2541.423583984375  -- Error: 0.5626478791236877 -- k: 9.892332077026367\n",
      "Epoch: 4600 -- Total Loss: 2337.95849609375  -- Data Loss: 0.6457003355026245 -- PDE Loss: 2303.68701171875  -- Error: 0.6050731539726257 -- k: 9.892643928527832\n",
      "Epoch: 4800 -- Total Loss: 2112.137939453125  -- Data Loss: 0.6094174981117249 -- PDE Loss: 2078.549560546875  -- Error: 0.5888461470603943 -- k: 9.892412185668945\n",
      "Training at resolution: 200x200\n",
      "Epoch: 0 -- Total Loss: 4350.87158203125  -- Data Loss: 0.5888873338699341 -- PDE Loss: 4317.64013671875  -- Error: 0.6717397570610046 -- k: 9.892157554626465\n",
      "Epoch: 200 -- Total Loss: 1622.8812255859375  -- Data Loss: 0.473276287317276 -- PDE Loss: 1591.9959716796875  -- Error: 0.5409297347068787 -- k: 9.892341613769531\n",
      "Epoch: 400 -- Total Loss: 2275.457275390625  -- Data Loss: 0.46056488156318665 -- PDE Loss: 2244.80517578125  -- Error: 0.5349123477935791 -- k: 9.892200469970703\n",
      "Epoch: 600 -- Total Loss: 2093.368896484375  -- Data Loss: 0.49440494179725647 -- PDE Loss: 2062.009765625  -- Error: 0.5377532243728638 -- k: 9.892072677612305\n",
      "Epoch: 800 -- Total Loss: 1260.395263671875  -- Data Loss: 0.4880058169364929 -- PDE Loss: 1229.158935546875  -- Error: 0.49642080068588257 -- k: 9.892045974731445\n",
      "Epoch: 1000 -- Total Loss: 1670.3541259765625  -- Data Loss: 0.46604064106941223 -- PDE Loss: 1639.5350341796875  -- Error: 0.4838252663612366 -- k: 9.891942977905273\n",
      "Epoch: 1200 -- Total Loss: 1532.0426025390625  -- Data Loss: 0.4430808424949646 -- PDE Loss: 1501.6973876953125  -- Error: 0.5007138252258301 -- k: 9.892004013061523\n",
      "Epoch: 1400 -- Total Loss: 970.6627197265625  -- Data Loss: 0.45230963826179504 -- PDE Loss: 940.162841796875  -- Error: 0.47586002945899963 -- k: 9.892138481140137\n",
      "Epoch: 1600 -- Total Loss: 728.6314086914062  -- Data Loss: 0.47595515847206116 -- PDE Loss: 697.6315307617188  -- Error: 0.5302969217300415 -- k: 9.892021179199219\n",
      "Epoch: 1800 -- Total Loss: 1609.5521240234375  -- Data Loss: 0.49886006116867065 -- PDE Loss: 1578.0855712890625  -- Error: 0.5098850727081299 -- k: 9.891973495483398\n",
      "Epoch: 2000 -- Total Loss: 1276.4581298828125  -- Data Loss: 0.4455234706401825 -- PDE Loss: 1246.065673828125  -- Error: 0.49559256434440613 -- k: 9.892030715942383\n",
      "Epoch: 2200 -- Total Loss: 1304.218017578125  -- Data Loss: 0.4390801191329956 -- PDE Loss: 1273.981201171875  -- Error: 0.49100735783576965 -- k: 9.89215087890625\n",
      "Epoch: 2400 -- Total Loss: 921.7265625  -- Data Loss: 0.4007098078727722 -- PDE Loss: 892.2462158203125  -- Error: 0.4571191072463989 -- k: 9.892085075378418\n",
      "Epoch: 2600 -- Total Loss: 1359.3287353515625  -- Data Loss: 0.4642133414745331 -- PDE Loss: 1328.6146240234375  -- Error: 0.5013392567634583 -- k: 9.892266273498535\n",
      "Epoch: 2800 -- Total Loss: 848.5779418945312  -- Data Loss: 0.4290744662284851 -- PDE Loss: 818.5673217773438  -- Error: 0.49562814831733704 -- k: 9.89228630065918\n",
      "Epoch: 3000 -- Total Loss: 1119.2484130859375  -- Data Loss: 0.409117728471756 -- PDE Loss: 1089.6480712890625  -- Error: 0.5077407956123352 -- k: 9.89234733581543\n",
      "Epoch: 3200 -- Total Loss: 1482.357666015625  -- Data Loss: 0.4121693968772888 -- PDE Loss: 1452.7332763671875  -- Error: 0.4887518286705017 -- k: 9.892508506774902\n",
      "Epoch: 3400 -- Total Loss: 2065.498779296875  -- Data Loss: 0.47864702343940735 -- PDE Loss: 2034.5233154296875  -- Error: 0.5236488580703735 -- k: 9.892417907714844\n",
      "Epoch: 3600 -- Total Loss: 1110.4019775390625  -- Data Loss: 0.4399297535419464 -- PDE Loss: 1080.204345703125  -- Error: 0.5314176082611084 -- k: 9.89243221282959\n",
      "Epoch: 3800 -- Total Loss: 1140.5869140625  -- Data Loss: 0.4080820083618164 -- PDE Loss: 1111.1026611328125  -- Error: 0.49490389227867126 -- k: 9.892823219299316\n",
      "Epoch: 4000 -- Total Loss: 1570.4853515625  -- Data Loss: 0.4001210927963257 -- PDE Loss: 1541.17138671875  -- Error: 0.5449037551879883 -- k: 9.892875671386719\n",
      "Epoch: 4200 -- Total Loss: 784.1941528320312  -- Data Loss: 0.44461196660995483 -- PDE Loss: 754.0001220703125  -- Error: 0.4838644862174988 -- k: 9.892913818359375\n",
      "Epoch: 4400 -- Total Loss: 1027.2720947265625  -- Data Loss: 0.3888225257396698 -- PDE Loss: 998.206298828125  -- Error: 0.504579484462738 -- k: 9.892970085144043\n",
      "Epoch: 4600 -- Total Loss: 1065.9610595703125  -- Data Loss: 0.41330572962760925 -- PDE Loss: 1036.4495849609375  -- Error: 0.5007445812225342 -- k: 9.893196105957031\n",
      "Epoch: 4800 -- Total Loss: 742.654296875  -- Data Loss: 0.45417264103889465 -- PDE Loss: 712.3216552734375  -- Error: 0.49747174978256226 -- k: 9.89319133758545\n",
      "Training at resolution: 400x400\n",
      "Epoch: 0 -- Total Loss: 980.8370361328125  -- Data Loss: 0.3995303809642792 -- PDE Loss: 951.5902099609375  -- Error: 0.5336595177650452 -- k: 9.89315128326416\n",
      "Epoch: 200 -- Total Loss: 792.9828491210938  -- Data Loss: 0.40344828367233276 -- PDE Loss: 763.6995239257812  -- Error: 0.4851018190383911 -- k: 9.89336109161377\n",
      "Epoch: 400 -- Total Loss: 615.2288818359375  -- Data Loss: 0.4014631509780884 -- PDE Loss: 586.0335083007812  -- Error: 0.48670223355293274 -- k: 9.89359188079834\n",
      "Epoch: 600 -- Total Loss: 800.52392578125  -- Data Loss: 0.4131835401058197 -- PDE Loss: 771.1238403320312  -- Error: 0.4552898108959198 -- k: 9.893744468688965\n",
      "Epoch: 800 -- Total Loss: 673.3353881835938  -- Data Loss: 0.4121125340461731 -- PDE Loss: 643.9417114257812  -- Error: 0.46069830656051636 -- k: 9.893682479858398\n",
      "Epoch: 1000 -- Total Loss: 487.1971435546875  -- Data Loss: 0.38783252239227295 -- PDE Loss: 458.32806396484375  -- Error: 0.4628908932209015 -- k: 9.893857955932617\n",
      "Epoch: 1200 -- Total Loss: 783.1790771484375  -- Data Loss: 0.3763926327228546 -- PDE Loss: 754.5858764648438  -- Error: 0.45455503463745117 -- k: 9.894110679626465\n",
      "Epoch: 1400 -- Total Loss: 780.0874633789062  -- Data Loss: 0.410328209400177 -- PDE Loss: 750.8482055664062  -- Error: 0.48656216263771057 -- k: 9.894270896911621\n",
      "Epoch: 1600 -- Total Loss: 687.67919921875  -- Data Loss: 0.3787491023540497 -- PDE Loss: 659.1184692382812  -- Error: 0.4577461779117584 -- k: 9.894509315490723\n",
      "Epoch: 1800 -- Total Loss: 546.0914306640625  -- Data Loss: 0.35685843229293823 -- PDE Loss: 517.9873657226562  -- Error: 0.45282885432243347 -- k: 9.89462947845459\n",
      "Epoch: 2000 -- Total Loss: 615.2958374023438  -- Data Loss: 0.38707494735717773 -- PDE Loss: 586.6058959960938  -- Error: 0.46034467220306396 -- k: 9.894712448120117\n",
      "Epoch: 2200 -- Total Loss: 582.9158935546875  -- Data Loss: 0.38636165857315063 -- PDE Loss: 554.2868041992188  -- Error: 0.49488410353660583 -- k: 9.894929885864258\n",
      "Epoch: 2400 -- Total Loss: 689.0607299804688  -- Data Loss: 0.3519725203514099 -- PDE Loss: 661.1522216796875  -- Error: 0.41931283473968506 -- k: 9.895105361938477\n",
      "Epoch: 2600 -- Total Loss: 691.6285400390625  -- Data Loss: 0.3565446138381958 -- PDE Loss: 663.6303100585938  -- Error: 0.45687633752822876 -- k: 9.895103454589844\n",
      "Epoch: 2800 -- Total Loss: 598.1553955078125  -- Data Loss: 0.36545342206954956 -- PDE Loss: 570.0377197265625  -- Error: 0.44379115104675293 -- k: 9.895415306091309\n",
      "Epoch: 3000 -- Total Loss: 575.2280883789062  -- Data Loss: 0.36980149149894714 -- PDE Loss: 547.0559692382812  -- Error: 0.44987908005714417 -- k: 9.895581245422363\n",
      "Epoch: 3200 -- Total Loss: 503.83221435546875  -- Data Loss: 0.4083062410354614 -- PDE Loss: 474.90216064453125  -- Error: 0.43373382091522217 -- k: 9.895627975463867\n",
      "Epoch: 3400 -- Total Loss: 773.9542846679688  -- Data Loss: 0.35228681564331055 -- PDE Loss: 746.18798828125  -- Error: 0.4489298462867737 -- k: 9.895844459533691\n",
      "Epoch: 3600 -- Total Loss: 425.5249328613281  -- Data Loss: 0.35141274333000183 -- PDE Loss: 397.8272705078125  -- Error: 0.4396677613258362 -- k: 9.896111488342285\n",
      "Epoch: 3800 -- Total Loss: 470.0860290527344  -- Data Loss: 0.34788933396339417 -- PDE Loss: 442.4986572265625  -- Error: 0.44207653403282166 -- k: 9.896316528320312\n",
      "Epoch: 4000 -- Total Loss: 471.3482666015625  -- Data Loss: 0.35293012857437134 -- PDE Loss: 443.7002868652344  -- Error: 0.4365364909172058 -- k: 9.89653205871582\n",
      "Epoch: 4200 -- Total Loss: 592.1941528320312  -- Data Loss: 0.34581810235977173 -- PDE Loss: 564.7440795898438  -- Error: 0.4403744339942932 -- k: 9.896801948547363\n",
      "Epoch: 4400 -- Total Loss: 429.3648376464844  -- Data Loss: 0.3361492156982422 -- PDE Loss: 402.1405944824219  -- Error: 0.41853389143943787 -- k: 9.896957397460938\n",
      "Epoch: 4600 -- Total Loss: 455.4908447265625  -- Data Loss: 0.3474293649196625 -- PDE Loss: 428.059326171875  -- Error: 0.4227091670036316 -- k: 9.897048950195312\n",
      "Epoch: 4800 -- Total Loss: 591.9990234375  -- Data Loss: 0.375774621963501 -- PDE Loss: 564.020751953125  -- Error: 0.45407041907310486 -- k: 9.89716625213623\n",
      "Training at resolution: 800x800\n",
      "Epoch: 0 -- Total Loss: 543.2010498046875  -- Data Loss: 0.317073792219162 -- PDE Loss: 516.4337768554688  -- Error: 0.4247036576271057 -- k: 9.897347450256348\n",
      "Epoch: 200 -- Total Loss: 222.45697021484375  -- Data Loss: 0.33854183554649353 -- PDE Loss: 195.35317993164062  -- Error: 0.40884825587272644 -- k: 9.897811889648438\n",
      "Epoch: 400 -- Total Loss: 185.4682159423828  -- Data Loss: 0.349962055683136 -- PDE Loss: 158.1818084716797  -- Error: 0.3877236545085907 -- k: 9.898050308227539\n",
      "Epoch: 600 -- Total Loss: 367.32415771484375  -- Data Loss: 0.3290438950061798 -- PDE Loss: 340.5245666503906  -- Error: 0.4362032115459442 -- k: 9.898391723632812\n",
      "Epoch: 800 -- Total Loss: 228.64901733398438  -- Data Loss: 0.29429492354393005 -- PDE Loss: 202.5973358154297  -- Error: 0.38469383120536804 -- k: 9.898656845092773\n",
      "Epoch: 1000 -- Total Loss: 480.63140869140625  -- Data Loss: 0.34182024002075195 -- PDE Loss: 453.70794677734375  -- Error: 0.4174637198448181 -- k: 9.899036407470703\n",
      "Epoch: 1200 -- Total Loss: 349.7104797363281  -- Data Loss: 0.32603269815444946 -- PDE Loss: 323.1371765136719  -- Error: 0.39433035254478455 -- k: 9.899222373962402\n",
      "Epoch: 1400 -- Total Loss: 255.24334716796875  -- Data Loss: 0.28760620951652527 -- PDE Loss: 229.4823760986328  -- Error: 0.3770143687725067 -- k: 9.899447441101074\n",
      "Epoch: 1600 -- Total Loss: 450.15118408203125  -- Data Loss: 0.31784379482269287 -- PDE Loss: 423.84710693359375  -- Error: 0.4140224754810333 -- k: 9.899773597717285\n",
      "Epoch: 1800 -- Total Loss: 271.85809326171875  -- Data Loss: 0.3007810711860657 -- PDE Loss: 245.94537353515625  -- Error: 0.37105050683021545 -- k: 9.900014877319336\n",
      "Epoch: 2000 -- Total Loss: 284.56463623046875  -- Data Loss: 0.32773369550704956 -- PDE Loss: 258.1728515625  -- Error: 0.402618408203125 -- k: 9.900321006774902\n",
      "Epoch: 2200 -- Total Loss: 447.5423583984375  -- Data Loss: 0.3123452961444855 -- PDE Loss: 421.548583984375  -- Error: 0.4132111072540283 -- k: 9.90076732635498\n",
      "Epoch: 2400 -- Total Loss: 501.34222412109375  -- Data Loss: 0.31381112337112427 -- PDE Loss: 475.3749084472656  -- Error: 0.3895375728607178 -- k: 9.90106201171875\n",
      "Epoch: 2600 -- Total Loss: 620.7172241210938  -- Data Loss: 0.345377117395401 -- PDE Loss: 594.1878051757812  -- Error: 0.4154971241950989 -- k: 9.901402473449707\n",
      "Epoch: 2800 -- Total Loss: 395.34393310546875  -- Data Loss: 0.3086546063423157 -- PDE Loss: 369.60595703125  -- Error: 0.40198129415512085 -- k: 9.901690483093262\n",
      "Epoch: 3000 -- Total Loss: 195.53501892089844  -- Data Loss: 0.295393705368042 -- PDE Loss: 170.12100219726562  -- Error: 0.37630411982536316 -- k: 9.901988983154297\n",
      "Epoch: 3200 -- Total Loss: 392.5970458984375  -- Data Loss: 0.31093594431877136 -- PDE Loss: 366.9290771484375  -- Error: 0.39005663990974426 -- k: 9.9022798538208\n",
      "Epoch: 3400 -- Total Loss: 534.4411010742188  -- Data Loss: 0.3177827000617981 -- PDE Loss: 508.70611572265625  -- Error: 0.3850916624069214 -- k: 9.902620315551758\n",
      "Epoch: 3600 -- Total Loss: 179.84864807128906  -- Data Loss: 0.2767219841480255 -- PDE Loss: 154.9862823486328  -- Error: 0.357578843832016 -- k: 9.902881622314453\n",
      "Epoch: 3800 -- Total Loss: 541.8953247070312  -- Data Loss: 0.3125613033771515 -- PDE Loss: 516.3539428710938  -- Error: 0.39074134826660156 -- k: 9.90306568145752\n",
      "Epoch: 4000 -- Total Loss: 440.24749755859375  -- Data Loss: 0.30041059851646423 -- PDE Loss: 415.04150390625  -- Error: 0.38392215967178345 -- k: 9.903553009033203\n",
      "Epoch: 4200 -- Total Loss: 366.0917663574219  -- Data Loss: 0.3127477467060089 -- PDE Loss: 340.70062255859375  -- Error: 0.402619868516922 -- k: 9.90385913848877\n",
      "Epoch: 4400 -- Total Loss: 653.6736450195312  -- Data Loss: 0.29818564653396606 -- PDE Loss: 628.5955200195312  -- Error: 0.3895515501499176 -- k: 9.903960227966309\n",
      "Epoch: 4600 -- Total Loss: 241.7302703857422  -- Data Loss: 0.3297464847564697 -- PDE Loss: 216.11190795898438  -- Error: 0.38261494040489197 -- k: 9.904436111450195\n",
      "Epoch: 4800 -- Total Loss: 239.03988647460938  -- Data Loss: 0.28016895055770874 -- PDE Loss: 214.49615478515625  -- Error: 0.3786347806453705 -- k: 9.904850959777832\n",
      "Training at resolution: 1000x1000\n",
      "Epoch: 0 -- Total Loss: 347.84033203125  -- Data Loss: 0.27525368332862854 -- PDE Loss: 323.439697265625  -- Error: 0.39264705777168274 -- k: 9.90507698059082\n",
      "Epoch: 200 -- Total Loss: 356.43524169921875  -- Data Loss: 0.2818569839000702 -- PDE Loss: 331.98583984375  -- Error: 0.3704950511455536 -- k: 9.90549373626709\n",
      "Epoch: 400 -- Total Loss: 131.70333862304688  -- Data Loss: 0.26355496048927307 -- PDE Loss: 107.66498565673828  -- Error: 0.36055421829223633 -- k: 9.905722618103027\n",
      "Epoch: 600 -- Total Loss: 178.04287719726562  -- Data Loss: 0.281401127576828 -- PDE Loss: 153.71726989746094  -- Error: 0.36089178919792175 -- k: 9.906071662902832\n",
      "Epoch: 800 -- Total Loss: 339.1254577636719  -- Data Loss: 0.25723397731781006 -- PDE Loss: 315.37176513671875  -- Error: 0.37584975361824036 -- k: 9.906523704528809\n",
      "Epoch: 1000 -- Total Loss: 519.5250244140625  -- Data Loss: 0.27804550528526306 -- PDE Loss: 495.41510009765625  -- Error: 0.4331211447715759 -- k: 9.906827926635742\n",
      "Epoch: 1200 -- Total Loss: 169.29762268066406  -- Data Loss: 0.27616846561431885 -- PDE Loss: 145.3007354736328  -- Error: 0.34524455666542053 -- k: 9.907205581665039\n",
      "Epoch: 1400 -- Total Loss: 234.76783752441406  -- Data Loss: 0.2562306523323059 -- PDE Loss: 211.25340270996094  -- Error: 0.3521789610385895 -- k: 9.907624244689941\n",
      "Epoch: 1600 -- Total Loss: 194.0452117919922  -- Data Loss: 0.25821977853775024 -- PDE Loss: 170.56370544433594  -- Error: 0.3573683798313141 -- k: 9.907987594604492\n",
      "Epoch: 1800 -- Total Loss: 282.02398681640625  -- Data Loss: 0.2789114713668823 -- PDE Loss: 258.20745849609375  -- Error: 0.38283780217170715 -- k: 9.908392906188965\n",
      "Epoch: 2000 -- Total Loss: 226.60182189941406  -- Data Loss: 0.26291733980178833 -- PDE Loss: 203.20497131347656  -- Error: 0.372386634349823 -- k: 9.90890121459961\n",
      "Epoch: 2200 -- Total Loss: 192.7440948486328  -- Data Loss: 0.277279794216156 -- PDE Loss: 169.1262969970703  -- Error: 0.37478873133659363 -- k: 9.9092435836792\n",
      "Epoch: 2400 -- Total Loss: 219.57504272460938  -- Data Loss: 0.25713321566581726 -- PDE Loss: 196.44256591796875  -- Error: 0.34821072220802307 -- k: 9.909640312194824\n",
      "Epoch: 2600 -- Total Loss: 244.2445526123047  -- Data Loss: 0.2659778892993927 -- PDE Loss: 221.00140380859375  -- Error: 0.3844783902168274 -- k: 9.90998363494873\n",
      "Epoch: 2800 -- Total Loss: 256.72625732421875  -- Data Loss: 0.2653641700744629 -- PDE Loss: 233.5604705810547  -- Error: 0.38724789023399353 -- k: 9.910319328308105\n",
      "Epoch: 3000 -- Total Loss: 319.77813720703125  -- Data Loss: 0.2662898302078247 -- PDE Loss: 296.6916198730469  -- Error: 0.3579872250556946 -- k: 9.910810470581055\n",
      "Epoch: 3200 -- Total Loss: 433.029541015625  -- Data Loss: 0.26803192496299744 -- PDE Loss: 409.9781799316406  -- Error: 0.36658141016960144 -- k: 9.911153793334961\n",
      "Epoch: 3400 -- Total Loss: 305.0870361328125  -- Data Loss: 0.24267616868019104 -- PDE Loss: 282.63397216796875  -- Error: 0.34149298071861267 -- k: 9.911599159240723\n",
      "Epoch: 3600 -- Total Loss: 102.41031646728516  -- Data Loss: 0.25122374296188354 -- PDE Loss: 79.86699676513672  -- Error: 0.34537452459335327 -- k: 9.912017822265625\n",
      "Epoch: 3800 -- Total Loss: 131.64297485351562  -- Data Loss: 0.25411638617515564 -- PDE Loss: 109.10084533691406  -- Error: 0.3293582797050476 -- k: 9.912318229675293\n",
      "Epoch: 4000 -- Total Loss: 175.84970092773438  -- Data Loss: 0.239481583237648 -- PDE Loss: 153.67176818847656  -- Error: 0.3390120565891266 -- k: 9.912677764892578\n",
      "Epoch: 4200 -- Total Loss: 315.38299560546875  -- Data Loss: 0.2487705498933792 -- PDE Loss: 293.0954895019531  -- Error: 0.351533442735672 -- k: 9.913064002990723\n",
      "Epoch: 4400 -- Total Loss: 161.13409423828125  -- Data Loss: 0.2358698695898056 -- PDE Loss: 139.183349609375  -- Error: 0.3441343903541565 -- k: 9.913470268249512\n",
      "Epoch: 4600 -- Total Loss: 254.9656982421875  -- Data Loss: 0.24118876457214355 -- PDE Loss: 232.98936462402344  -- Error: 0.35553601384162903 -- k: 9.913847923278809\n",
      "Epoch: 4800 -- Total Loss: 233.9521942138672  -- Data Loss: 0.24693097174167633 -- PDE Loss: 211.94189453125  -- Error: 0.3553701341152191 -- k: 9.914298057556152\n",
      "Training at resolution: 1200x1200\n",
      "Epoch: 0 -- Total Loss: 245.73541259765625  -- Data Loss: 0.234079048037529 -- PDE Loss: 224.07492065429688  -- Error: 0.35625842213630676 -- k: 9.914732933044434\n",
      "Epoch: 200 -- Total Loss: 107.42276763916016  -- Data Loss: 0.241348996758461 -- PDE Loss: 85.68683624267578  -- Error: 0.34197527170181274 -- k: 9.915096282958984\n",
      "Epoch: 400 -- Total Loss: 169.03919982910156  -- Data Loss: 0.24924632906913757 -- PDE Loss: 147.21437072753906  -- Error: 0.333085298538208 -- k: 9.915449142456055\n",
      "Epoch: 600 -- Total Loss: 99.28954315185547  -- Data Loss: 0.2244110405445099 -- PDE Loss: 78.0440444946289  -- Error: 0.3126387298107147 -- k: 9.915862083435059\n",
      "Epoch: 800 -- Total Loss: 128.85433959960938  -- Data Loss: 0.24821847677230835 -- PDE Loss: 107.2170639038086  -- Error: 0.3393862247467041 -- k: 9.91628360748291\n",
      "Epoch: 1000 -- Total Loss: 240.09938049316406  -- Data Loss: 0.2167702317237854 -- PDE Loss: 219.19834899902344  -- Error: 0.3297530710697174 -- k: 9.916836738586426\n",
      "Epoch: 1200 -- Total Loss: 244.54998779296875  -- Data Loss: 0.2172342985868454 -- PDE Loss: 223.73756408691406  -- Error: 0.32804715633392334 -- k: 9.917317390441895\n",
      "Epoch: 1400 -- Total Loss: 87.0206069946289  -- Data Loss: 0.22313591837882996 -- PDE Loss: 66.16293334960938  -- Error: 0.32403621077537537 -- k: 9.91769027709961\n",
      "Epoch: 1600 -- Total Loss: 107.18456268310547  -- Data Loss: 0.2313520312309265 -- PDE Loss: 86.24183654785156  -- Error: 0.31660783290863037 -- k: 9.918088912963867\n",
      "Epoch: 1800 -- Total Loss: 93.70771026611328  -- Data Loss: 0.2172815501689911 -- PDE Loss: 73.11878967285156  -- Error: 0.30666548013687134 -- k: 9.918450355529785\n",
      "Epoch: 2000 -- Total Loss: 198.7012481689453  -- Data Loss: 0.21393564343452454 -- PDE Loss: 178.26416015625  -- Error: 0.316307008266449 -- k: 9.918889045715332\n",
      "Epoch: 2200 -- Total Loss: 214.4337615966797  -- Data Loss: 0.2084478735923767 -- PDE Loss: 194.18760681152344  -- Error: 0.3139673173427582 -- k: 9.919285774230957\n",
      "Epoch: 2400 -- Total Loss: 344.82977294921875  -- Data Loss: 0.2298242598772049 -- PDE Loss: 324.2483215332031  -- Error: 0.3574041724205017 -- k: 9.919755935668945\n",
      "Epoch: 2600 -- Total Loss: 232.3718719482422  -- Data Loss: 0.21542002260684967 -- PDE Loss: 212.1515350341797  -- Error: 0.3434053063392639 -- k: 9.920137405395508\n"
     ]
    }
   ],
   "source": [
    "ks = [5, 7, 10]\n",
    "#k = 1\n",
    "# Defina as resoluções em ordem crescente\n",
    "resolutions=[100, 200, 400, 800, 1000, 1200] # Malhas de baixa a alta resolução\n",
    "#resolutions = [100, 200]\n",
    "epoch = 5000\n",
    "epoch_levels = len(resolutions)*epoch\n",
    "for k in ks:\n",
    "    #resolutions=[100, 200, 400] if k < 5 else [800, 1000, 1200]\n",
    "    state.params[\"params\"][\"ksq\"] = jnp.array((k+0.1)**2)\n",
    "    hmt, ret = multigrid_training(state=state, rng_key=rng_key, k=k, \n",
    "                                     resolutions=resolutions, n_ep = epoch, xdom=[xmin, xmax], ydom=[ymin, ymax])\n",
    "    \n",
    "    x, y, t, u_true, u_pred, q, k_pred = hmt[\"x\"], hmt[\"y\"], hmt[\"timelapse\"], hmt[\"u_true\"], hmt[\"u_max\"]*hmt[\"u_pred\"], hmt[\"forcing_term\"], hmt[\"k_pred\"]\n",
    "    \n",
    "    helmholtz_init_graf(x,y,u_true,q)\n",
    "    helmholtz_graf(x, y, u_true, u_pred, k, k_pred, epoch_levels, t)\n",
    "    helmholtz_error_graf(ret[\"total_loss_log\"], ret[\"data_loss_log\"], ret[\"pde_loss_log\"], ret[\"error_log\"], ret[\"k_log\"], k, epoch_levels)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
